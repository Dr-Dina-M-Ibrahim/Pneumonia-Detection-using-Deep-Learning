{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pneumnia deep learning model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYXiFRtTF3oY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgpm2tLcxeEk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7553aea8-ded2-4e08-ac5c-aa1ef2425ff3"
      },
      "source": [
        "#load and preprocessing the augmanted images using image data generator\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Directory with our training Pneumonia pictures\n",
        "train_Pneumonia = os.path.join('gdrive/My Drive/dataset/train')\n",
        "\n",
        "# Create image generator to resize, normalize, filter, and augment the images\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      validation_split=0.3,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_Pneumonia,  # This is the source directory for training images\n",
        "        target_size=(224, 224),  # All images will be resized to 224x224\n",
        "        batch_size=64,\n",
        "        subset='training',\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 32 using validation_datagen generator\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        train_Pneumonia,  # This is the source directory for validation images\n",
        "        target_size=(224, 224),  # All images will be resized to 224x224\n",
        "        batch_size=32,\n",
        "        subset='validation',\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 14600 images belonging to 2 classes.\n",
            "Found 6255 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LRrkg7ONKOH5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "d8645e47-dd46-4699-8487-e5dd1ceff052"
      },
      "source": [
        "#ResNet152V2 model for Pneumnia deep learning detection\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.applications import ResNet152V2\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Lambda, MaxPooling2D,AveragePooling2D# convolution layers\n",
        "from keras.layers import Dense, Dropout, Flatten,Activation,Reshape # core layers1\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "def createModel(): #ResNet152V2 model for Pneumnia deep learning detection\n",
        "   top_model = Sequential()\n",
        "   top_model.add(ResNet152V2(include_top=False,weights=\"imagenet\",\n",
        "                             input_shape=(224,224,3)))\n",
        "   top_model.add(Reshape((7,7,2048), input_shape=(-1,7,7,2048)))\n",
        "   top_model.add(Flatten())\n",
        "   top_model.add(Dense(256, activation='relu'))\n",
        "   top_model.add(Dropout(0.5))\n",
        "   top_model.add(Dense(1, activation='sigmoid'))\n",
        "   return top_model\n",
        "model = createModel()\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet152v2 (Model)          (None, 7, 7, 2048)        58331648  \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 7, 7, 2048)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 100352)            0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               25690368  \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 84,022,273\n",
            "Trainable params: 83,878,529\n",
            "Non-trainable params: 143,744\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lb0vihnGGU0m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f1db8ecb-13b1-4ec8-a7c3-08dabced070b"
      },
      "source": [
        "#MobileNetV2 model for Pneumnia deep learning detection\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.applications import ResNet152V2,MobileNetV2\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Lambda, MaxPooling2D,AveragePooling2D# convolution layers\n",
        "from keras.layers import Dense, Dropout, Flatten,Activation,Reshape # core layers1\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "def createModel(): #MobileNetV2 model for Pneumnia deep learning detection\n",
        "   top_model = Sequential()\n",
        "   top_model.add(MobileNetV2(include_top=False,weights=\"imagenet\",\n",
        "                             input_shape=(224,224,3)))\n",
        "   top_model.add(Reshape((7,7,1280), input_shape=(-1,7,7,1280)))\n",
        "   top_model.add(Flatten())\n",
        "   top_model.add(Dense(512, activation='relu'))\n",
        "   top_model.add(Dropout(0.5))\n",
        "   top_model.add(Dense(1, activation='sigmoid'))\n",
        "   return top_model\n",
        "model = createModel()\n",
        "\n",
        "model.summary()\n",
        "#plot the model as a graph\n",
        "keras.utils.plot_model(model, \"my_first_model.png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "mobilenetv2_1.00_224 (Model) (None, 7, 7, 1280)        2257984   \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 7, 7, 1280)        0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 62720)             0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 512)               32113152  \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 34,371,649\n",
            "Trainable params: 34,337,537\n",
            "Non-trainable params: 34,112\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEvTQ6VwK5iE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "34807b9e-52b2-44f5-96a7-18da4635d65c"
      },
      "source": [
        "#LSTM-CNN code\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,Conv1D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.layers import Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation, Reshape\n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "import sys\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from keras.models import load_model\n",
        "from keras import regularizers\n",
        "from keras.layers import Dense, Dropout, Flatten,Activation \n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.layers import InputLayer\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from keras.optimizers import SGD, Adam\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def crate_Model(): #LSTM-CNN code model for Pneumnia deep learning detection\n",
        "    model = Sequential()\n",
        "    model.add(BatchNormalization(input_shape=(200,200,3)))\n",
        "    print(\"shape is {}\".format(model.output_shape))\n",
        "    model.add(TimeDistributed(LSTM(128, activation='tanh',\n",
        "                                   return_sequences = True)))\n",
        "    model.add(Dropout(0.6))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(TimeDistributed(Conv1D(128, kernel_size=4, activation='relu',\n",
        "                                     padding=\"same\",\n",
        "                                     kernel_regularizer=regularizers.l2(0.01), \n",
        "                                     bias_regularizer=regularizers.l2(0.01))))\n",
        "    model.add(AveragePooling2D((2,2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "    print(model.summary())\n",
        "  \n",
        "    return model\n",
        "model7=crate_Model()\n",
        "\n",
        "#model1.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape is (None, 200, 200, 3)\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization_7 (Batch (None, 200, 200, 3)       12        \n",
            "_________________________________________________________________\n",
            "time_distributed_9 (TimeDist (None, 200, 200, 128)     67584     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 200, 200, 128)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 200, 200, 128)     512       \n",
            "_________________________________________________________________\n",
            "time_distributed_10 (TimeDis (None, 200, 200, 128)     65664     \n",
            "_________________________________________________________________\n",
            "average_pooling2d_4 (Average (None, 100, 100, 128)     0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 1280000)           0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 1280000)           0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               163840128 \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 163,974,029\n",
            "Trainable params: 163,973,767\n",
            "Non-trainable params: 262\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ekpw6NqHrGxo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "6a9801d6-2df9-4127-8a7b-1a53491ea56d"
      },
      "source": [
        "#CNN model \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras import regularizers\n",
        "from tensorflow.keras import initializers\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential() #CNN code model for Pneumnia deep learning detection\n",
        "model.add(Conv2D(16, (3, 3), input_shape=(224, 224, 3),padding=\"same\", \n",
        "                 kernel_initializer='random_normal',bias_initializer='zeros', \n",
        "                 kernel_regularizer=regularizers.l2(0.02), \n",
        "                 bias_regularizer=regularizers.l2(0.02)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(32, (3, 3),padding=\"same\", kernel_initializer='random_normal',\n",
        "                 bias_initializer='zeros', \n",
        "                 kernel_regularizer=regularizers.l2(0.02), \n",
        "                 bias_regularizer=regularizers.l2(0.02)))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(3, 3),strides=(3,3)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(MaxPooling2D(pool_size=(2, 2),strides=(3,3)))\n",
        "\n",
        "model.add(Conv2D(128, (2,2), activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),strides=(3,3)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "\n",
        "model.add(Dense(512,activation='relu')\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(1000))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "model7=model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-e33c8a39e99d>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    model.add(Dense(1000))\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fczdVw4ORoT_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4c705e1e-248d-4a9b-9939-d7e523531d46"
      },
      "source": [
        "#Optimizer and fit function\n",
        "from tensorflow.keras.optimizers import RMSprop, Adamax\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.optimizers import SGD, Adam, Adamax\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "opt = Adamax(lr= 0.00006, beta_1=0.9, beta_2=0.999, epsilon=1e-07)   #adam = Adam(lr=0.0001), momentum=0.9 Adamax(lr= 0.00045, beta_1=0.9, beta_2=0.999, epsilon=1e-07)  \n",
        "#opt = keras.optimizers.Adam(learning_rate=0.07)\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
        "\n",
        "model8.compile(loss='binary_crossentropy',\n",
        "              optimizer= opt,\n",
        "              metrics=METRICS)\n",
        "\n",
        "history = model8.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=8,\n",
        "      epochs=200,\n",
        "      verbose=1,\n",
        "      validation_data = validation_generator,\n",
        "     \n",
        "      validation_steps=8) \n",
        "      \n",
        "\n",
        "#Save and serialize\n",
        "model8.save(\"path_to_my_model_lstm_cnn_89\")\n",
        "model8.save('my_model_lstm_cnn_89.h5')\n",
        "print('model saved!!!!!')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.8620 - tp: 145.0000 - fp: 53.0000 - tn: 33.0000 - fn: 25.0000 - accuracy: 0.6953 - precision: 0.7323 - recall: 0.8529 - auc: 0.7039 - val_loss: 0.7167 - val_tp: 143.0000 - val_fp: 42.0000 - val_tn: 60.0000 - val_fn: 11.0000 - val_accuracy: 0.7930 - val_precision: 0.7730 - val_recall: 0.9286 - val_auc: 0.9161\n",
            "Epoch 2/200\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.8871 - tp: 125.0000 - fp: 65.0000 - tn: 44.0000 - fn: 22.0000 - accuracy: 0.6602 - precision: 0.6579 - recall: 0.8503 - auc: 0.7188 - val_loss: 0.7300 - val_tp: 133.0000 - val_fp: 22.0000 - val_tn: 73.0000 - val_fn: 28.0000 - val_accuracy: 0.8047 - val_precision: 0.8581 - val_recall: 0.8261 - val_auc: 0.8952\n",
            "Epoch 3/200\n",
            "8/8 [==============================] - 21s 3s/step - loss: 0.8553 - tp: 121.0000 - fp: 46.0000 - tn: 47.0000 - fn: 42.0000 - accuracy: 0.6562 - precision: 0.7246 - recall: 0.7423 - auc: 0.7192 - val_loss: 0.7499 - val_tp: 111.0000 - val_fp: 17.0000 - val_tn: 98.0000 - val_fn: 30.0000 - val_accuracy: 0.8164 - val_precision: 0.8672 - val_recall: 0.7872 - val_auc: 0.8747\n",
            "Epoch 4/200\n",
            "8/8 [==============================] - 21s 3s/step - loss: 0.8475 - tp: 129.0000 - fp: 46.0000 - tn: 53.0000 - fn: 28.0000 - accuracy: 0.7109 - precision: 0.7371 - recall: 0.8217 - auc: 0.7620 - val_loss: 0.6917 - val_tp: 134.0000 - val_fp: 6.0000 - val_tn: 86.0000 - val_fn: 30.0000 - val_accuracy: 0.8594 - val_precision: 0.9571 - val_recall: 0.8171 - val_auc: 0.9467\n",
            "Epoch 5/200\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.8210 - tp: 127.0000 - fp: 38.0000 - tn: 50.0000 - fn: 41.0000 - accuracy: 0.6914 - precision: 0.7697 - recall: 0.7560 - auc: 0.7540 - val_loss: 0.7183 - val_tp: 138.0000 - val_fp: 14.0000 - val_tn: 79.0000 - val_fn: 25.0000 - val_accuracy: 0.8477 - val_precision: 0.9079 - val_recall: 0.8466 - val_auc: 0.9124\n",
            "Epoch 6/200\n",
            "8/8 [==============================] - 18s 2s/step - loss: 0.8218 - tp: 128.0000 - fp: 56.0000 - tn: 48.0000 - fn: 24.0000 - accuracy: 0.6875 - precision: 0.6957 - recall: 0.8421 - auc: 0.7704 - val_loss: 0.7129 - val_tp: 136.0000 - val_fp: 17.0000 - val_tn: 80.0000 - val_fn: 23.0000 - val_accuracy: 0.8438 - val_precision: 0.8889 - val_recall: 0.8553 - val_auc: 0.9111\n",
            "Epoch 7/200\n",
            "8/8 [==============================] - 29s 4s/step - loss: 0.8347 - tp: 125.0000 - fp: 52.0000 - tn: 42.0000 - fn: 37.0000 - accuracy: 0.6523 - precision: 0.7062 - recall: 0.7716 - auc: 0.7516 - val_loss: 0.7114 - val_tp: 138.0000 - val_fp: 20.0000 - val_tn: 83.0000 - val_fn: 15.0000 - val_accuracy: 0.8633 - val_precision: 0.8734 - val_recall: 0.9020 - val_auc: 0.9291\n",
            "Epoch 8/200\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.7892 - tp: 130.0000 - fp: 33.0000 - tn: 64.0000 - fn: 29.0000 - accuracy: 0.7578 - precision: 0.7975 - recall: 0.8176 - auc: 0.8035 - val_loss: 0.7288 - val_tp: 137.0000 - val_fp: 14.0000 - val_tn: 76.0000 - val_fn: 29.0000 - val_accuracy: 0.8320 - val_precision: 0.9073 - val_recall: 0.8253 - val_auc: 0.8909\n",
            "Epoch 9/200\n",
            "8/8 [==============================] - 20s 2s/step - loss: 0.8188 - tp: 127.0000 - fp: 47.0000 - tn: 60.0000 - fn: 22.0000 - accuracy: 0.7305 - precision: 0.7299 - recall: 0.8523 - auc: 0.7937 - val_loss: 0.7019 - val_tp: 148.0000 - val_fp: 13.0000 - val_tn: 85.0000 - val_fn: 10.0000 - val_accuracy: 0.9102 - val_precision: 0.9193 - val_recall: 0.9367 - val_auc: 0.9387\n",
            "Epoch 10/200\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.7921 - tp: 123.0000 - fp: 46.0000 - tn: 60.0000 - fn: 27.0000 - accuracy: 0.7148 - precision: 0.7278 - recall: 0.8200 - auc: 0.8085 - val_loss: 0.7116 - val_tp: 141.0000 - val_fp: 20.0000 - val_tn: 77.0000 - val_fn: 18.0000 - val_accuracy: 0.8516 - val_precision: 0.8758 - val_recall: 0.8868 - val_auc: 0.9241\n",
            "Epoch 11/200\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.7683 - tp: 136.0000 - fp: 39.0000 - tn: 50.0000 - fn: 31.0000 - accuracy: 0.7266 - precision: 0.7771 - recall: 0.8144 - auc: 0.8006 - val_loss: 0.6952 - val_tp: 126.0000 - val_fp: 18.0000 - val_tn: 97.0000 - val_fn: 15.0000 - val_accuracy: 0.8711 - val_precision: 0.8750 - val_recall: 0.8936 - val_auc: 0.9438\n",
            "Epoch 12/200\n",
            "8/8 [==============================] - 20s 2s/step - loss: 0.7927 - tp: 143.0000 - fp: 47.0000 - tn: 41.0000 - fn: 25.0000 - accuracy: 0.7188 - precision: 0.7526 - recall: 0.8512 - auc: 0.7709 - val_loss: 0.7122 - val_tp: 146.0000 - val_fp: 15.0000 - val_tn: 69.0000 - val_fn: 26.0000 - val_accuracy: 0.8398 - val_precision: 0.9068 - val_recall: 0.8488 - val_auc: 0.8958\n",
            "Epoch 13/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.8492 - tp: 138.0000 - fp: 45.0000 - tn: 47.0000 - fn: 26.0000 - accuracy: 0.7227 - precision: 0.7541 - recall: 0.8415 - auc: 0.7383 - val_loss: 0.6888 - val_tp: 150.0000 - val_fp: 23.0000 - val_tn: 67.0000 - val_fn: 16.0000 - val_accuracy: 0.8477 - val_precision: 0.8671 - val_recall: 0.9036 - val_auc: 0.9167\n",
            "Epoch 14/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.7783 - tp: 140.0000 - fp: 53.0000 - tn: 47.0000 - fn: 16.0000 - accuracy: 0.7305 - precision: 0.7254 - recall: 0.8974 - auc: 0.8204 - val_loss: 0.6906 - val_tp: 152.0000 - val_fp: 18.0000 - val_tn: 69.0000 - val_fn: 17.0000 - val_accuracy: 0.8633 - val_precision: 0.8941 - val_recall: 0.8994 - val_auc: 0.9178\n",
            "Epoch 15/200\n",
            "8/8 [==============================] - 20s 2s/step - loss: 0.7647 - tp: 141.0000 - fp: 40.0000 - tn: 52.0000 - fn: 23.0000 - accuracy: 0.7539 - precision: 0.7790 - recall: 0.8598 - auc: 0.8193 - val_loss: 0.6906 - val_tp: 144.0000 - val_fp: 22.0000 - val_tn: 75.0000 - val_fn: 15.0000 - val_accuracy: 0.8555 - val_precision: 0.8675 - val_recall: 0.9057 - val_auc: 0.9278\n",
            "Epoch 16/200\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.7927 - tp: 140.0000 - fp: 42.0000 - tn: 53.0000 - fn: 21.0000 - accuracy: 0.7539 - precision: 0.7692 - recall: 0.8696 - auc: 0.7930 - val_loss: 0.6628 - val_tp: 154.0000 - val_fp: 22.0000 - val_tn: 70.0000 - val_fn: 10.0000 - val_accuracy: 0.8750 - val_precision: 0.8750 - val_recall: 0.9390 - val_auc: 0.9476\n",
            "Epoch 17/200\n",
            "8/8 [==============================] - 20s 3s/step - loss: 0.8073 - tp: 133.0000 - fp: 42.0000 - tn: 58.0000 - fn: 23.0000 - accuracy: 0.7461 - precision: 0.7600 - recall: 0.8526 - auc: 0.7945 - val_loss: 0.7069 - val_tp: 137.0000 - val_fp: 40.0000 - val_tn: 66.0000 - val_fn: 13.0000 - val_accuracy: 0.7930 - val_precision: 0.7740 - val_recall: 0.9133 - val_auc: 0.9087\n",
            "Epoch 18/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.7240 - tp: 133.0000 - fp: 38.0000 - tn: 65.0000 - fn: 20.0000 - accuracy: 0.7734 - precision: 0.7778 - recall: 0.8693 - auc: 0.8599 - val_loss: 0.6982 - val_tp: 130.0000 - val_fp: 40.0000 - val_tn: 75.0000 - val_fn: 11.0000 - val_accuracy: 0.8008 - val_precision: 0.7647 - val_recall: 0.9220 - val_auc: 0.9378\n",
            "Epoch 19/200\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.8097 - tp: 126.0000 - fp: 45.0000 - tn: 58.0000 - fn: 27.0000 - accuracy: 0.7188 - precision: 0.7368 - recall: 0.8235 - auc: 0.7852 - val_loss: 0.7035 - val_tp: 140.0000 - val_fp: 25.0000 - val_tn: 78.0000 - val_fn: 13.0000 - val_accuracy: 0.8516 - val_precision: 0.8485 - val_recall: 0.9150 - val_auc: 0.9158\n",
            "Epoch 20/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.7789 - tp: 130.0000 - fp: 49.0000 - tn: 58.0000 - fn: 19.0000 - accuracy: 0.7344 - precision: 0.7263 - recall: 0.8725 - auc: 0.8219 - val_loss: 0.6590 - val_tp: 147.0000 - val_fp: 21.0000 - val_tn: 73.0000 - val_fn: 15.0000 - val_accuracy: 0.8594 - val_precision: 0.8750 - val_recall: 0.9074 - val_auc: 0.9368\n",
            "Epoch 21/200\n",
            "8/8 [==============================] - 18s 2s/step - loss: 0.7764 - tp: 136.0000 - fp: 48.0000 - tn: 43.0000 - fn: 29.0000 - accuracy: 0.6992 - precision: 0.7391 - recall: 0.8242 - auc: 0.7995 - val_loss: 0.6795 - val_tp: 142.0000 - val_fp: 17.0000 - val_tn: 77.0000 - val_fn: 20.0000 - val_accuracy: 0.8555 - val_precision: 0.8931 - val_recall: 0.8765 - val_auc: 0.9292\n",
            "Epoch 22/200\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.8032 - tp: 130.0000 - fp: 47.0000 - tn: 59.0000 - fn: 20.0000 - accuracy: 0.7383 - precision: 0.7345 - recall: 0.8667 - auc: 0.8068 - val_loss: 0.6762 - val_tp: 143.0000 - val_fp: 26.0000 - val_tn: 78.0000 - val_fn: 9.0000 - val_accuracy: 0.8633 - val_precision: 0.8462 - val_recall: 0.9408 - val_auc: 0.9373\n",
            "Epoch 23/200\n",
            "8/8 [==============================] - 18s 2s/step - loss: 0.7946 - tp: 131.0000 - fp: 43.0000 - tn: 54.0000 - fn: 28.0000 - accuracy: 0.7227 - precision: 0.7529 - recall: 0.8239 - auc: 0.7878 - val_loss: 0.6603 - val_tp: 142.0000 - val_fp: 27.0000 - val_tn: 76.0000 - val_fn: 11.0000 - val_accuracy: 0.8516 - val_precision: 0.8402 - val_recall: 0.9281 - val_auc: 0.9380\n",
            "Epoch 24/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.8423 - tp: 125.0000 - fp: 56.0000 - tn: 57.0000 - fn: 18.0000 - accuracy: 0.7109 - precision: 0.6906 - recall: 0.8741 - auc: 0.7819 - val_loss: 0.6787 - val_tp: 141.0000 - val_fp: 19.0000 - val_tn: 80.0000 - val_fn: 16.0000 - val_accuracy: 0.8633 - val_precision: 0.8813 - val_recall: 0.8981 - val_auc: 0.9300\n",
            "Epoch 25/200\n",
            "8/8 [==============================] - 18s 2s/step - loss: 0.8051 - tp: 127.0000 - fp: 39.0000 - tn: 60.0000 - fn: 30.0000 - accuracy: 0.7305 - precision: 0.7651 - recall: 0.8089 - auc: 0.7982 - val_loss: 0.6765 - val_tp: 137.0000 - val_fp: 14.0000 - val_tn: 82.0000 - val_fn: 23.0000 - val_accuracy: 0.8555 - val_precision: 0.9073 - val_recall: 0.8562 - val_auc: 0.9263\n",
            "Epoch 26/200\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.8298 - tp: 128.0000 - fp: 44.0000 - tn: 51.0000 - fn: 33.0000 - accuracy: 0.6992 - precision: 0.7442 - recall: 0.7950 - auc: 0.7532 - val_loss: 0.7072 - val_tp: 131.0000 - val_fp: 21.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8594 - val_precision: 0.8618 - val_recall: 0.8973 - val_auc: 0.9229\n",
            "Epoch 27/200\n",
            "8/8 [==============================] - 18s 2s/step - loss: 0.7859 - tp: 130.0000 - fp: 39.0000 - tn: 62.0000 - fn: 25.0000 - accuracy: 0.7500 - precision: 0.7692 - recall: 0.8387 - auc: 0.8044 - val_loss: 0.6410 - val_tp: 157.0000 - val_fp: 24.0000 - val_tn: 69.0000 - val_fn: 6.0000 - val_accuracy: 0.8828 - val_precision: 0.8674 - val_recall: 0.9632 - val_auc: 0.9506\n",
            "Epoch 28/200\n",
            "8/8 [==============================] - 18s 2s/step - loss: 0.7715 - tp: 140.0000 - fp: 37.0000 - tn: 52.0000 - fn: 27.0000 - accuracy: 0.7500 - precision: 0.7910 - recall: 0.8383 - auc: 0.8085 - val_loss: 0.6532 - val_tp: 151.0000 - val_fp: 29.0000 - val_tn: 62.0000 - val_fn: 14.0000 - val_accuracy: 0.8320 - val_precision: 0.8389 - val_recall: 0.9152 - val_auc: 0.9306\n",
            "Epoch 29/200\n",
            "8/8 [==============================] - 18s 2s/step - loss: 0.7648 - tp: 136.0000 - fp: 43.0000 - tn: 60.0000 - fn: 17.0000 - accuracy: 0.7656 - precision: 0.7598 - recall: 0.8889 - auc: 0.8328 - val_loss: 0.6599 - val_tp: 139.0000 - val_fp: 25.0000 - val_tn: 81.0000 - val_fn: 11.0000 - val_accuracy: 0.8594 - val_precision: 0.8476 - val_recall: 0.9267 - val_auc: 0.9483\n",
            "Epoch 30/200\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.7234 - tp: 140.0000 - fp: 30.0000 - tn: 62.0000 - fn: 24.0000 - accuracy: 0.7891 - precision: 0.8235 - recall: 0.8537 - auc: 0.8542 - val_loss: 0.6450 - val_tp: 155.0000 - val_fp: 23.0000 - val_tn: 68.0000 - val_fn: 10.0000 - val_accuracy: 0.8711 - val_precision: 0.8708 - val_recall: 0.9394 - val_auc: 0.9416\n",
            "Epoch 31/200\n",
            "8/8 [==============================] - 18s 2s/step - loss: 0.7519 - tp: 142.0000 - fp: 37.0000 - tn: 52.0000 - fn: 25.0000 - accuracy: 0.7578 - precision: 0.7933 - recall: 0.8503 - auc: 0.8204 - val_loss: 0.6320 - val_tp: 155.0000 - val_fp: 17.0000 - val_tn: 70.0000 - val_fn: 14.0000 - val_accuracy: 0.8789 - val_precision: 0.9012 - val_recall: 0.9172 - val_auc: 0.9529\n",
            "Epoch 32/200\n",
            "8/8 [==============================] - 18s 2s/step - loss: 0.7655 - tp: 142.0000 - fp: 30.0000 - tn: 48.0000 - fn: 36.0000 - accuracy: 0.7422 - precision: 0.8256 - recall: 0.7978 - auc: 0.7971 - val_loss: 0.6785 - val_tp: 142.0000 - val_fp: 22.0000 - val_tn: 72.0000 - val_fn: 20.0000 - val_accuracy: 0.8359 - val_precision: 0.8659 - val_recall: 0.8765 - val_auc: 0.9129\n",
            "Epoch 33/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.8118 - tp: 131.0000 - fp: 48.0000 - tn: 53.0000 - fn: 24.0000 - accuracy: 0.7188 - precision: 0.7318 - recall: 0.8452 - auc: 0.7914 - val_loss: 0.6449 - val_tp: 143.0000 - val_fp: 24.0000 - val_tn: 74.0000 - val_fn: 15.0000 - val_accuracy: 0.8477 - val_precision: 0.8563 - val_recall: 0.9051 - val_auc: 0.9467\n",
            "Epoch 34/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.7193 - tp: 147.0000 - fp: 34.0000 - tn: 54.0000 - fn: 21.0000 - accuracy: 0.7852 - precision: 0.8122 - recall: 0.8750 - auc: 0.8506 - val_loss: 0.6327 - val_tp: 153.0000 - val_fp: 7.0000 - val_tn: 74.0000 - val_fn: 22.0000 - val_accuracy: 0.8867 - val_precision: 0.9563 - val_recall: 0.8743 - val_auc: 0.9370\n",
            "Epoch 35/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.7392 - tp: 142.0000 - fp: 44.0000 - tn: 53.0000 - fn: 17.0000 - accuracy: 0.7617 - precision: 0.7634 - recall: 0.8931 - auc: 0.8413 - val_loss: 0.6361 - val_tp: 146.0000 - val_fp: 23.0000 - val_tn: 74.0000 - val_fn: 13.0000 - val_accuracy: 0.8594 - val_precision: 0.8639 - val_recall: 0.9182 - val_auc: 0.9498\n",
            "Epoch 36/200\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.7726 - tp: 139.0000 - fp: 43.0000 - tn: 52.0000 - fn: 22.0000 - accuracy: 0.7461 - precision: 0.7637 - recall: 0.8634 - auc: 0.8137 - val_loss: 0.6347 - val_tp: 153.0000 - val_fp: 20.0000 - val_tn: 70.0000 - val_fn: 13.0000 - val_accuracy: 0.8711 - val_precision: 0.8844 - val_recall: 0.9217 - val_auc: 0.9422\n",
            "Epoch 37/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.7628 - tp: 140.0000 - fp: 39.0000 - tn: 57.0000 - fn: 20.0000 - accuracy: 0.7695 - precision: 0.7821 - recall: 0.8750 - auc: 0.8196 - val_loss: 0.6475 - val_tp: 139.0000 - val_fp: 21.0000 - val_tn: 82.0000 - val_fn: 14.0000 - val_accuracy: 0.8633 - val_precision: 0.8687 - val_recall: 0.9085 - val_auc: 0.9453\n",
            "Epoch 38/200\n",
            "8/8 [==============================] - 18s 2s/step - loss: 0.7791 - tp: 136.0000 - fp: 50.0000 - tn: 49.0000 - fn: 21.0000 - accuracy: 0.7227 - precision: 0.7312 - recall: 0.8662 - auc: 0.8098 - val_loss: 0.6500 - val_tp: 127.0000 - val_fp: 16.0000 - val_tn: 102.0000 - val_fn: 11.0000 - val_accuracy: 0.8945 - val_precision: 0.8881 - val_recall: 0.9203 - val_auc: 0.9574\n",
            "Epoch 39/200\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.8475 - tp: 124.0000 - fp: 51.0000 - tn: 59.0000 - fn: 22.0000 - accuracy: 0.7148 - precision: 0.7086 - recall: 0.8493 - auc: 0.7757 - val_loss: 0.6770 - val_tp: 125.0000 - val_fp: 20.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8477 - val_precision: 0.8621 - val_recall: 0.8681 - val_auc: 0.9106\n",
            "Epoch 40/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.7835 - tp: 125.0000 - fp: 40.0000 - tn: 65.0000 - fn: 26.0000 - accuracy: 0.7422 - precision: 0.7576 - recall: 0.8278 - auc: 0.8230 - val_loss: 0.6671 - val_tp: 127.0000 - val_fp: 10.0000 - val_tn: 94.0000 - val_fn: 25.0000 - val_accuracy: 0.8633 - val_precision: 0.9270 - val_recall: 0.8355 - val_auc: 0.9365\n",
            "Epoch 41/200\n",
            "8/8 [==============================] - 18s 2s/step - loss: 0.7622 - tp: 133.0000 - fp: 38.0000 - tn: 59.0000 - fn: 26.0000 - accuracy: 0.7500 - precision: 0.7778 - recall: 0.8365 - auc: 0.8255 - val_loss: 0.6362 - val_tp: 155.0000 - val_fp: 12.0000 - val_tn: 67.0000 - val_fn: 22.0000 - val_accuracy: 0.8672 - val_precision: 0.9281 - val_recall: 0.8757 - val_auc: 0.9348\n",
            "Epoch 42/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.7332 - tp: 138.0000 - fp: 33.0000 - tn: 59.0000 - fn: 26.0000 - accuracy: 0.7695 - precision: 0.8070 - recall: 0.8415 - auc: 0.8390 - val_loss: 0.6639 - val_tp: 136.0000 - val_fp: 16.0000 - val_tn: 84.0000 - val_fn: 20.0000 - val_accuracy: 0.8594 - val_precision: 0.8947 - val_recall: 0.8718 - val_auc: 0.9294\n",
            "Epoch 43/200\n",
            "8/8 [==============================] - 20s 2s/step - loss: 0.7157 - tp: 144.0000 - fp: 28.0000 - tn: 55.0000 - fn: 29.0000 - accuracy: 0.7773 - precision: 0.8372 - recall: 0.8324 - auc: 0.8432 - val_loss: 0.6447 - val_tp: 134.0000 - val_fp: 16.0000 - val_tn: 87.0000 - val_fn: 19.0000 - val_accuracy: 0.8633 - val_precision: 0.8933 - val_recall: 0.8758 - val_auc: 0.9417\n",
            "Epoch 44/200\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.7456 - tp: 131.0000 - fp: 36.0000 - tn: 68.0000 - fn: 21.0000 - accuracy: 0.7773 - precision: 0.7844 - recall: 0.8618 - auc: 0.8393 - val_loss: 0.6245 - val_tp: 145.0000 - val_fp: 11.0000 - val_tn: 84.0000 - val_fn: 16.0000 - val_accuracy: 0.8945 - val_precision: 0.9295 - val_recall: 0.9006 - val_auc: 0.9547\n",
            "Epoch 45/200\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.7455 - tp: 126.0000 - fp: 38.0000 - tn: 68.0000 - fn: 24.0000 - accuracy: 0.7578 - precision: 0.7683 - recall: 0.8400 - auc: 0.8383 - val_loss: 0.6100 - val_tp: 143.0000 - val_fp: 11.0000 - val_tn: 89.0000 - val_fn: 13.0000 - val_accuracy: 0.9062 - val_precision: 0.9286 - val_recall: 0.9167 - val_auc: 0.9608\n",
            "Epoch 46/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.7055 - tp: 145.0000 - fp: 26.0000 - tn: 63.0000 - fn: 22.0000 - accuracy: 0.8125 - precision: 0.8480 - recall: 0.8683 - auc: 0.8737 - val_loss: 0.6783 - val_tp: 127.0000 - val_fp: 22.0000 - val_tn: 85.0000 - val_fn: 22.0000 - val_accuracy: 0.8281 - val_precision: 0.8523 - val_recall: 0.8523 - val_auc: 0.9076\n",
            "Epoch 47/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.7560 - tp: 140.0000 - fp: 31.0000 - tn: 55.0000 - fn: 30.0000 - accuracy: 0.7617 - precision: 0.8187 - recall: 0.8235 - auc: 0.8159 - val_loss: 0.6563 - val_tp: 147.0000 - val_fp: 19.0000 - val_tn: 73.0000 - val_fn: 17.0000 - val_accuracy: 0.8594 - val_precision: 0.8855 - val_recall: 0.8963 - val_auc: 0.9296\n",
            "Epoch 48/200\n",
            "8/8 [==============================] - 24s 3s/step - loss: 0.7654 - tp: 126.0000 - fp: 41.0000 - tn: 65.0000 - fn: 24.0000 - accuracy: 0.7461 - precision: 0.7545 - recall: 0.8400 - auc: 0.8298 - val_loss: 0.6154 - val_tp: 153.0000 - val_fp: 18.0000 - val_tn: 72.0000 - val_fn: 13.0000 - val_accuracy: 0.8789 - val_precision: 0.8947 - val_recall: 0.9217 - val_auc: 0.9478\n",
            "Epoch 49/200\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.8341 - tp: 124.0000 - fp: 49.0000 - tn: 54.0000 - fn: 29.0000 - accuracy: 0.6953 - precision: 0.7168 - recall: 0.8105 - auc: 0.7749 - val_loss: 0.6281 - val_tp: 138.0000 - val_fp: 15.0000 - val_tn: 93.0000 - val_fn: 10.0000 - val_accuracy: 0.9023 - val_precision: 0.9020 - val_recall: 0.9324 - val_auc: 0.9524\n",
            "Epoch 50/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.7914 - tp: 125.0000 - fp: 33.0000 - tn: 65.0000 - fn: 33.0000 - accuracy: 0.7422 - precision: 0.7911 - recall: 0.7911 - auc: 0.8180 - val_loss: 0.6256 - val_tp: 150.0000 - val_fp: 13.0000 - val_tn: 78.0000 - val_fn: 15.0000 - val_accuracy: 0.8906 - val_precision: 0.9202 - val_recall: 0.9091 - val_auc: 0.9396\n",
            "Epoch 51/200\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.7090 - tp: 147.0000 - fp: 27.0000 - tn: 55.0000 - fn: 27.0000 - accuracy: 0.7891 - precision: 0.8448 - recall: 0.8448 - auc: 0.8504 - val_loss: 0.6561 - val_tp: 150.0000 - val_fp: 15.0000 - val_tn: 72.0000 - val_fn: 19.0000 - val_accuracy: 0.8672 - val_precision: 0.9091 - val_recall: 0.8876 - val_auc: 0.9204\n",
            "Epoch 52/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.7878 - tp: 135.0000 - fp: 41.0000 - tn: 56.0000 - fn: 24.0000 - accuracy: 0.7461 - precision: 0.7670 - recall: 0.8491 - auc: 0.8079 - val_loss: 0.6614 - val_tp: 142.0000 - val_fp: 29.0000 - val_tn: 73.0000 - val_fn: 12.0000 - val_accuracy: 0.8398 - val_precision: 0.8304 - val_recall: 0.9221 - val_auc: 0.9351\n",
            "Epoch 53/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7670 - tp: 145.0000 - fp: 36.0000 - tn: 48.0000 - fn: 27.0000 - accuracy: 0.7539 - precision: 0.8011 - recall: 0.8430 - auc: 0.7906 - val_loss: 0.6371 - val_tp: 150.0000 - val_fp: 15.0000 - val_tn: 75.0000 - val_fn: 16.0000 - val_accuracy: 0.8789 - val_precision: 0.9091 - val_recall: 0.9036 - val_auc: 0.9387\n",
            "Epoch 54/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7651 - tp: 126.0000 - fp: 44.0000 - tn: 69.0000 - fn: 17.0000 - accuracy: 0.7617 - precision: 0.7412 - recall: 0.8811 - auc: 0.8453 - val_loss: 0.6279 - val_tp: 155.0000 - val_fp: 15.0000 - val_tn: 71.0000 - val_fn: 15.0000 - val_accuracy: 0.8828 - val_precision: 0.9118 - val_recall: 0.9118 - val_auc: 0.9433\n",
            "Epoch 55/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.7579 - tp: 144.0000 - fp: 34.0000 - tn: 55.0000 - fn: 23.0000 - accuracy: 0.7773 - precision: 0.8090 - recall: 0.8623 - auc: 0.8277 - val_loss: 0.6647 - val_tp: 123.0000 - val_fp: 14.0000 - val_tn: 92.0000 - val_fn: 27.0000 - val_accuracy: 0.8398 - val_precision: 0.8978 - val_recall: 0.8200 - val_auc: 0.9324\n",
            "Epoch 56/200\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.7798 - tp: 138.0000 - fp: 44.0000 - tn: 55.0000 - fn: 19.0000 - accuracy: 0.7539 - precision: 0.7582 - recall: 0.8790 - auc: 0.8136 - val_loss: 0.6480 - val_tp: 145.0000 - val_fp: 10.0000 - val_tn: 81.0000 - val_fn: 20.0000 - val_accuracy: 0.8828 - val_precision: 0.9355 - val_recall: 0.8788 - val_auc: 0.9416\n",
            "Epoch 57/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.7281 - tp: 137.0000 - fp: 39.0000 - tn: 56.0000 - fn: 24.0000 - accuracy: 0.7539 - precision: 0.7784 - recall: 0.8509 - auc: 0.8430 - val_loss: 0.6520 - val_tp: 124.0000 - val_fp: 10.0000 - val_tn: 99.0000 - val_fn: 23.0000 - val_accuracy: 0.8711 - val_precision: 0.9254 - val_recall: 0.8435 - val_auc: 0.9470\n",
            "Epoch 58/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.7878 - tp: 129.0000 - fp: 43.0000 - tn: 51.0000 - fn: 33.0000 - accuracy: 0.7031 - precision: 0.7500 - recall: 0.7963 - auc: 0.7784 - val_loss: 0.6904 - val_tp: 135.0000 - val_fp: 4.0000 - val_tn: 81.0000 - val_fn: 36.0000 - val_accuracy: 0.8438 - val_precision: 0.9712 - val_recall: 0.7895 - val_auc: 0.9329\n",
            "Epoch 59/200\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.7547 - tp: 130.0000 - fp: 43.0000 - tn: 53.0000 - fn: 30.0000 - accuracy: 0.7148 - precision: 0.7514 - recall: 0.8125 - auc: 0.8142 - val_loss: 0.6619 - val_tp: 129.0000 - val_fp: 12.0000 - val_tn: 90.0000 - val_fn: 25.0000 - val_accuracy: 0.8555 - val_precision: 0.9149 - val_recall: 0.8377 - val_auc: 0.9272\n",
            "Epoch 60/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.7259 - tp: 143.0000 - fp: 34.0000 - tn: 56.0000 - fn: 23.0000 - accuracy: 0.7773 - precision: 0.8079 - recall: 0.8614 - auc: 0.8488 - val_loss: 0.6484 - val_tp: 138.0000 - val_fp: 10.0000 - val_tn: 87.0000 - val_fn: 21.0000 - val_accuracy: 0.8789 - val_precision: 0.9324 - val_recall: 0.8679 - val_auc: 0.9347\n",
            "Epoch 61/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.7200 - tp: 143.0000 - fp: 38.0000 - tn: 55.0000 - fn: 20.0000 - accuracy: 0.7734 - precision: 0.7901 - recall: 0.8773 - auc: 0.8591 - val_loss: 0.6672 - val_tp: 127.0000 - val_fp: 5.0000 - val_tn: 86.0000 - val_fn: 38.0000 - val_accuracy: 0.8320 - val_precision: 0.9621 - val_recall: 0.7697 - val_auc: 0.9192\n",
            "Epoch 62/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.7859 - tp: 139.0000 - fp: 35.0000 - tn: 49.0000 - fn: 33.0000 - accuracy: 0.7344 - precision: 0.7989 - recall: 0.8081 - auc: 0.7940 - val_loss: 0.6564 - val_tp: 131.0000 - val_fp: 6.0000 - val_tn: 94.0000 - val_fn: 25.0000 - val_accuracy: 0.8789 - val_precision: 0.9562 - val_recall: 0.8397 - val_auc: 0.9422\n",
            "Epoch 63/200\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.8664 - tp: 121.0000 - fp: 50.0000 - tn: 56.0000 - fn: 29.0000 - accuracy: 0.6914 - precision: 0.7076 - recall: 0.8067 - auc: 0.7588 - val_loss: 0.6590 - val_tp: 128.0000 - val_fp: 4.0000 - val_tn: 91.0000 - val_fn: 33.0000 - val_accuracy: 0.8555 - val_precision: 0.9697 - val_recall: 0.7950 - val_auc: 0.9387\n",
            "Epoch 64/200\n",
            "8/8 [==============================] - 24s 3s/step - loss: 0.8033 - tp: 128.0000 - fp: 42.0000 - tn: 58.0000 - fn: 28.0000 - accuracy: 0.7266 - precision: 0.7529 - recall: 0.8205 - auc: 0.7922 - val_loss: 0.6345 - val_tp: 140.0000 - val_fp: 6.0000 - val_tn: 89.0000 - val_fn: 21.0000 - val_accuracy: 0.8945 - val_precision: 0.9589 - val_recall: 0.8696 - val_auc: 0.9575\n",
            "Epoch 65/200\n",
            "8/8 [==============================] - 20s 2s/step - loss: 0.7014 - tp: 148.0000 - fp: 29.0000 - tn: 61.0000 - fn: 18.0000 - accuracy: 0.8164 - precision: 0.8362 - recall: 0.8916 - auc: 0.8592 - val_loss: 0.6407 - val_tp: 142.0000 - val_fp: 6.0000 - val_tn: 83.0000 - val_fn: 25.0000 - val_accuracy: 0.8789 - val_precision: 0.9595 - val_recall: 0.8503 - val_auc: 0.9403\n",
            "Epoch 66/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.8160 - tp: 128.0000 - fp: 57.0000 - tn: 51.0000 - fn: 20.0000 - accuracy: 0.6992 - precision: 0.6919 - recall: 0.8649 - auc: 0.7941 - val_loss: 0.6590 - val_tp: 137.0000 - val_fp: 12.0000 - val_tn: 84.0000 - val_fn: 23.0000 - val_accuracy: 0.8633 - val_precision: 0.9195 - val_recall: 0.8562 - val_auc: 0.9378\n",
            "Epoch 67/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.7298 - tp: 144.0000 - fp: 35.0000 - tn: 45.0000 - fn: 32.0000 - accuracy: 0.7383 - precision: 0.8045 - recall: 0.8182 - auc: 0.8162 - val_loss: 0.6219 - val_tp: 147.0000 - val_fp: 8.0000 - val_tn: 81.0000 - val_fn: 20.0000 - val_accuracy: 0.8906 - val_precision: 0.9484 - val_recall: 0.8802 - val_auc: 0.9433\n",
            "Epoch 68/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7501 - tp: 135.0000 - fp: 34.0000 - tn: 65.0000 - fn: 22.0000 - accuracy: 0.7812 - precision: 0.7988 - recall: 0.8599 - auc: 0.8346 - val_loss: 0.6471 - val_tp: 141.0000 - val_fp: 10.0000 - val_tn: 84.0000 - val_fn: 21.0000 - val_accuracy: 0.8789 - val_precision: 0.9338 - val_recall: 0.8704 - val_auc: 0.9427\n",
            "Epoch 69/200\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.7459 - tp: 146.0000 - fp: 34.0000 - tn: 53.0000 - fn: 23.0000 - accuracy: 0.7773 - precision: 0.8111 - recall: 0.8639 - auc: 0.8087 - val_loss: 0.6135 - val_tp: 148.0000 - val_fp: 10.0000 - val_tn: 84.0000 - val_fn: 14.0000 - val_accuracy: 0.9062 - val_precision: 0.9367 - val_recall: 0.9136 - val_auc: 0.9648\n",
            "Epoch 70/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.8549 - tp: 123.0000 - fp: 53.0000 - tn: 57.0000 - fn: 23.0000 - accuracy: 0.7031 - precision: 0.6989 - recall: 0.8425 - auc: 0.7763 - val_loss: 0.6147 - val_tp: 146.0000 - val_fp: 11.0000 - val_tn: 83.0000 - val_fn: 16.0000 - val_accuracy: 0.8945 - val_precision: 0.9299 - val_recall: 0.9012 - val_auc: 0.9637\n",
            "Epoch 71/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.7824 - tp: 138.0000 - fp: 38.0000 - tn: 51.0000 - fn: 29.0000 - accuracy: 0.7383 - precision: 0.7841 - recall: 0.8263 - auc: 0.7880 - val_loss: 0.6516 - val_tp: 147.0000 - val_fp: 12.0000 - val_tn: 78.0000 - val_fn: 19.0000 - val_accuracy: 0.8789 - val_precision: 0.9245 - val_recall: 0.8855 - val_auc: 0.9287\n",
            "Epoch 72/200\n",
            "8/8 [==============================] - 20s 2s/step - loss: 0.7793 - tp: 135.0000 - fp: 37.0000 - tn: 63.0000 - fn: 21.0000 - accuracy: 0.7734 - precision: 0.7849 - recall: 0.8654 - auc: 0.8262 - val_loss: 0.6651 - val_tp: 130.0000 - val_fp: 13.0000 - val_tn: 87.0000 - val_fn: 26.0000 - val_accuracy: 0.8477 - val_precision: 0.9091 - val_recall: 0.8333 - val_auc: 0.9159\n",
            "Epoch 73/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.7222 - tp: 144.0000 - fp: 31.0000 - tn: 60.0000 - fn: 21.0000 - accuracy: 0.7969 - precision: 0.8229 - recall: 0.8727 - auc: 0.8484 - val_loss: 0.6317 - val_tp: 155.0000 - val_fp: 22.0000 - val_tn: 68.0000 - val_fn: 11.0000 - val_accuracy: 0.8711 - val_precision: 0.8757 - val_recall: 0.9337 - val_auc: 0.9421\n",
            "Epoch 74/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7437 - tp: 144.0000 - fp: 36.0000 - tn: 53.0000 - fn: 23.0000 - accuracy: 0.7695 - precision: 0.8000 - recall: 0.8623 - auc: 0.8285 - val_loss: 0.6350 - val_tp: 132.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8750 - val_precision: 0.9103 - val_recall: 0.8742 - val_auc: 0.9475\n",
            "Epoch 75/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.7530 - tp: 129.0000 - fp: 35.0000 - tn: 59.0000 - fn: 33.0000 - accuracy: 0.7344 - precision: 0.7866 - recall: 0.7963 - auc: 0.8201 - val_loss: 0.6473 - val_tp: 141.0000 - val_fp: 18.0000 - val_tn: 83.0000 - val_fn: 14.0000 - val_accuracy: 0.8750 - val_precision: 0.8868 - val_recall: 0.9097 - val_auc: 0.9358\n",
            "Epoch 76/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.7664 - tp: 114.0000 - fp: 41.0000 - tn: 80.0000 - fn: 21.0000 - accuracy: 0.7578 - precision: 0.7355 - recall: 0.8444 - auc: 0.8375 - val_loss: 0.6350 - val_tp: 140.0000 - val_fp: 4.0000 - val_tn: 85.0000 - val_fn: 27.0000 - val_accuracy: 0.8789 - val_precision: 0.9722 - val_recall: 0.8383 - val_auc: 0.9539\n",
            "Epoch 77/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7188 - tp: 142.0000 - fp: 30.0000 - tn: 56.0000 - fn: 28.0000 - accuracy: 0.7734 - precision: 0.8256 - recall: 0.8353 - auc: 0.8475 - val_loss: 0.6505 - val_tp: 144.0000 - val_fp: 5.0000 - val_tn: 80.0000 - val_fn: 27.0000 - val_accuracy: 0.8750 - val_precision: 0.9664 - val_recall: 0.8421 - val_auc: 0.9342\n",
            "Epoch 78/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.7534 - tp: 145.0000 - fp: 33.0000 - tn: 54.0000 - fn: 24.0000 - accuracy: 0.7773 - precision: 0.8146 - recall: 0.8580 - auc: 0.8209 - val_loss: 0.6136 - val_tp: 140.0000 - val_fp: 14.0000 - val_tn: 89.0000 - val_fn: 13.0000 - val_accuracy: 0.8945 - val_precision: 0.9091 - val_recall: 0.9150 - val_auc: 0.9620\n",
            "Epoch 79/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.7398 - tp: 141.0000 - fp: 35.0000 - tn: 53.0000 - fn: 27.0000 - accuracy: 0.7578 - precision: 0.8011 - recall: 0.8393 - auc: 0.8272 - val_loss: 0.6059 - val_tp: 145.0000 - val_fp: 12.0000 - val_tn: 81.0000 - val_fn: 18.0000 - val_accuracy: 0.8828 - val_precision: 0.9236 - val_recall: 0.8896 - val_auc: 0.9528\n",
            "Epoch 80/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7372 - tp: 134.0000 - fp: 35.0000 - tn: 64.0000 - fn: 23.0000 - accuracy: 0.7734 - precision: 0.7929 - recall: 0.8535 - auc: 0.8300 - val_loss: 0.6423 - val_tp: 139.0000 - val_fp: 19.0000 - val_tn: 78.0000 - val_fn: 20.0000 - val_accuracy: 0.8477 - val_precision: 0.8797 - val_recall: 0.8742 - val_auc: 0.9166\n",
            "Epoch 81/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7373 - tp: 139.0000 - fp: 29.0000 - tn: 61.0000 - fn: 27.0000 - accuracy: 0.7812 - precision: 0.8274 - recall: 0.8373 - auc: 0.8286 - val_loss: 0.6400 - val_tp: 145.0000 - val_fp: 21.0000 - val_tn: 75.0000 - val_fn: 15.0000 - val_accuracy: 0.8594 - val_precision: 0.8735 - val_recall: 0.9062 - val_auc: 0.9194\n",
            "Epoch 82/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7387 - tp: 141.0000 - fp: 40.0000 - tn: 56.0000 - fn: 19.0000 - accuracy: 0.7695 - precision: 0.7790 - recall: 0.8813 - auc: 0.8230 - val_loss: 0.6214 - val_tp: 135.0000 - val_fp: 20.0000 - val_tn: 82.0000 - val_fn: 19.0000 - val_accuracy: 0.8477 - val_precision: 0.8710 - val_recall: 0.8766 - val_auc: 0.9432\n",
            "Epoch 83/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.7431 - tp: 133.0000 - fp: 36.0000 - tn: 64.0000 - fn: 23.0000 - accuracy: 0.7695 - precision: 0.7870 - recall: 0.8526 - auc: 0.8311 - val_loss: 0.6496 - val_tp: 123.0000 - val_fp: 14.0000 - val_tn: 97.0000 - val_fn: 22.0000 - val_accuracy: 0.8594 - val_precision: 0.8978 - val_recall: 0.8483 - val_auc: 0.9284\n",
            "Epoch 84/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.8311 - tp: 123.0000 - fp: 46.0000 - tn: 62.0000 - fn: 25.0000 - accuracy: 0.7227 - precision: 0.7278 - recall: 0.8311 - auc: 0.7841 - val_loss: 0.6536 - val_tp: 144.0000 - val_fp: 13.0000 - val_tn: 74.0000 - val_fn: 25.0000 - val_accuracy: 0.8516 - val_precision: 0.9172 - val_recall: 0.8521 - val_auc: 0.9027\n",
            "Epoch 85/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7181 - tp: 143.0000 - fp: 32.0000 - tn: 57.0000 - fn: 24.0000 - accuracy: 0.7812 - precision: 0.8171 - recall: 0.8563 - auc: 0.8525 - val_loss: 0.6171 - val_tp: 141.0000 - val_fp: 17.0000 - val_tn: 83.0000 - val_fn: 15.0000 - val_accuracy: 0.8750 - val_precision: 0.8924 - val_recall: 0.9038 - val_auc: 0.9461\n",
            "Epoch 86/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7336 - tp: 141.0000 - fp: 36.0000 - tn: 57.0000 - fn: 22.0000 - accuracy: 0.7734 - precision: 0.7966 - recall: 0.8650 - auc: 0.8358 - val_loss: 0.6360 - val_tp: 139.0000 - val_fp: 15.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8789 - val_precision: 0.9026 - val_recall: 0.8968 - val_auc: 0.9352\n",
            "Epoch 87/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.7776 - tp: 130.0000 - fp: 32.0000 - tn: 59.0000 - fn: 35.0000 - accuracy: 0.7383 - precision: 0.8025 - recall: 0.7879 - auc: 0.8110 - val_loss: 0.6019 - val_tp: 160.0000 - val_fp: 21.0000 - val_tn: 63.0000 - val_fn: 12.0000 - val_accuracy: 0.8711 - val_precision: 0.8840 - val_recall: 0.9302 - val_auc: 0.9361\n",
            "Epoch 88/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.7151 - tp: 141.0000 - fp: 30.0000 - tn: 60.0000 - fn: 25.0000 - accuracy: 0.7852 - precision: 0.8246 - recall: 0.8494 - auc: 0.8500 - val_loss: 0.6444 - val_tp: 146.0000 - val_fp: 30.0000 - val_tn: 74.0000 - val_fn: 6.0000 - val_accuracy: 0.8594 - val_precision: 0.8295 - val_recall: 0.9605 - val_auc: 0.9351\n",
            "Epoch 89/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.7389 - tp: 144.0000 - fp: 46.0000 - tn: 50.0000 - fn: 16.0000 - accuracy: 0.7578 - precision: 0.7579 - recall: 0.9000 - auc: 0.8356 - val_loss: 0.6856 - val_tp: 137.0000 - val_fp: 35.0000 - val_tn: 70.0000 - val_fn: 14.0000 - val_accuracy: 0.8086 - val_precision: 0.7965 - val_recall: 0.9073 - val_auc: 0.8909\n",
            "Epoch 90/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.8089 - tp: 138.0000 - fp: 38.0000 - tn: 47.0000 - fn: 33.0000 - accuracy: 0.7227 - precision: 0.7841 - recall: 0.8070 - auc: 0.7573 - val_loss: 0.5752 - val_tp: 161.0000 - val_fp: 18.0000 - val_tn: 65.0000 - val_fn: 12.0000 - val_accuracy: 0.8828 - val_precision: 0.8994 - val_recall: 0.9306 - val_auc: 0.9597\n",
            "Epoch 91/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.7745 - tp: 121.0000 - fp: 42.0000 - tn: 67.0000 - fn: 26.0000 - accuracy: 0.7344 - precision: 0.7423 - recall: 0.8231 - auc: 0.8128 - val_loss: 0.6510 - val_tp: 141.0000 - val_fp: 17.0000 - val_tn: 85.0000 - val_fn: 13.0000 - val_accuracy: 0.8828 - val_precision: 0.8924 - val_recall: 0.9156 - val_auc: 0.9272\n",
            "Epoch 92/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7075 - tp: 135.0000 - fp: 37.0000 - tn: 61.0000 - fn: 23.0000 - accuracy: 0.7656 - precision: 0.7849 - recall: 0.8544 - auc: 0.8602 - val_loss: 0.6315 - val_tp: 137.0000 - val_fp: 11.0000 - val_tn: 81.0000 - val_fn: 27.0000 - val_accuracy: 0.8516 - val_precision: 0.9257 - val_recall: 0.8354 - val_auc: 0.9348\n",
            "Epoch 93/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.7373 - tp: 132.0000 - fp: 36.0000 - tn: 63.0000 - fn: 25.0000 - accuracy: 0.7617 - precision: 0.7857 - recall: 0.8408 - auc: 0.8350 - val_loss: 0.6223 - val_tp: 137.0000 - val_fp: 14.0000 - val_tn: 84.0000 - val_fn: 21.0000 - val_accuracy: 0.8633 - val_precision: 0.9073 - val_recall: 0.8671 - val_auc: 0.9284\n",
            "Epoch 94/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7349 - tp: 130.0000 - fp: 35.0000 - tn: 64.0000 - fn: 27.0000 - accuracy: 0.7578 - precision: 0.7879 - recall: 0.8280 - auc: 0.8362 - val_loss: 0.5831 - val_tp: 150.0000 - val_fp: 10.0000 - val_tn: 84.0000 - val_fn: 12.0000 - val_accuracy: 0.9141 - val_precision: 0.9375 - val_recall: 0.9259 - val_auc: 0.9693\n",
            "Epoch 95/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7210 - tp: 147.0000 - fp: 34.0000 - tn: 55.0000 - fn: 20.0000 - accuracy: 0.7891 - precision: 0.8122 - recall: 0.8802 - auc: 0.8476 - val_loss: 0.6200 - val_tp: 140.0000 - val_fp: 15.0000 - val_tn: 85.0000 - val_fn: 16.0000 - val_accuracy: 0.8789 - val_precision: 0.9032 - val_recall: 0.8974 - val_auc: 0.9460\n",
            "Epoch 96/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7764 - tp: 135.0000 - fp: 37.0000 - tn: 54.0000 - fn: 30.0000 - accuracy: 0.7383 - precision: 0.7849 - recall: 0.8182 - auc: 0.8002 - val_loss: 0.6161 - val_tp: 140.0000 - val_fp: 21.0000 - val_tn: 79.0000 - val_fn: 16.0000 - val_accuracy: 0.8555 - val_precision: 0.8696 - val_recall: 0.8974 - val_auc: 0.9408\n",
            "Epoch 97/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7965 - tp: 127.0000 - fp: 39.0000 - tn: 52.0000 - fn: 38.0000 - accuracy: 0.6992 - precision: 0.7651 - recall: 0.7697 - auc: 0.7734 - val_loss: 0.6229 - val_tp: 138.0000 - val_fp: 21.0000 - val_tn: 80.0000 - val_fn: 17.0000 - val_accuracy: 0.8516 - val_precision: 0.8679 - val_recall: 0.8903 - val_auc: 0.9372\n",
            "Epoch 98/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7653 - tp: 136.0000 - fp: 42.0000 - tn: 54.0000 - fn: 24.0000 - accuracy: 0.7422 - precision: 0.7640 - recall: 0.8500 - auc: 0.8202 - val_loss: 0.6267 - val_tp: 143.0000 - val_fp: 28.0000 - val_tn: 71.0000 - val_fn: 14.0000 - val_accuracy: 0.8359 - val_precision: 0.8363 - val_recall: 0.9108 - val_auc: 0.9297\n",
            "Epoch 99/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7374 - tp: 142.0000 - fp: 39.0000 - tn: 58.0000 - fn: 17.0000 - accuracy: 0.7812 - precision: 0.7845 - recall: 0.8931 - auc: 0.8418 - val_loss: 0.6263 - val_tp: 151.0000 - val_fp: 20.0000 - val_tn: 68.0000 - val_fn: 17.0000 - val_accuracy: 0.8555 - val_precision: 0.8830 - val_recall: 0.8988 - val_auc: 0.9252\n",
            "Epoch 100/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7159 - tp: 147.0000 - fp: 31.0000 - tn: 52.0000 - fn: 26.0000 - accuracy: 0.7773 - precision: 0.8258 - recall: 0.8497 - auc: 0.8300 - val_loss: 0.6025 - val_tp: 147.0000 - val_fp: 25.0000 - val_tn: 71.0000 - val_fn: 13.0000 - val_accuracy: 0.8516 - val_precision: 0.8547 - val_recall: 0.9187 - val_auc: 0.9471\n",
            "Epoch 101/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.8592 - tp: 126.0000 - fp: 46.0000 - tn: 54.0000 - fn: 30.0000 - accuracy: 0.7031 - precision: 0.7326 - recall: 0.8077 - auc: 0.7483 - val_loss: 0.6305 - val_tp: 142.0000 - val_fp: 31.0000 - val_tn: 73.0000 - val_fn: 10.0000 - val_accuracy: 0.8398 - val_precision: 0.8208 - val_recall: 0.9342 - val_auc: 0.9324\n",
            "Epoch 102/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.6970 - tp: 129.0000 - fp: 33.0000 - tn: 65.0000 - fn: 29.0000 - accuracy: 0.7578 - precision: 0.7963 - recall: 0.8165 - auc: 0.8570 - val_loss: 0.6627 - val_tp: 139.0000 - val_fp: 30.0000 - val_tn: 71.0000 - val_fn: 16.0000 - val_accuracy: 0.8203 - val_precision: 0.8225 - val_recall: 0.8968 - val_auc: 0.9049\n",
            "Epoch 103/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7443 - tp: 150.0000 - fp: 34.0000 - tn: 44.0000 - fn: 28.0000 - accuracy: 0.7578 - precision: 0.8152 - recall: 0.8427 - auc: 0.8060 - val_loss: 0.6145 - val_tp: 146.0000 - val_fp: 22.0000 - val_tn: 77.0000 - val_fn: 11.0000 - val_accuracy: 0.8711 - val_precision: 0.8690 - val_recall: 0.9299 - val_auc: 0.9338\n",
            "Epoch 104/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.7381 - tp: 133.0000 - fp: 40.0000 - tn: 60.0000 - fn: 23.0000 - accuracy: 0.7539 - precision: 0.7688 - recall: 0.8526 - auc: 0.8471 - val_loss: 0.6049 - val_tp: 148.0000 - val_fp: 25.0000 - val_tn: 70.0000 - val_fn: 13.0000 - val_accuracy: 0.8516 - val_precision: 0.8555 - val_recall: 0.9193 - val_auc: 0.9414\n",
            "Epoch 105/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.7418 - tp: 133.0000 - fp: 41.0000 - tn: 60.0000 - fn: 22.0000 - accuracy: 0.7539 - precision: 0.7644 - recall: 0.8581 - auc: 0.8271 - val_loss: 0.6348 - val_tp: 135.0000 - val_fp: 17.0000 - val_tn: 81.0000 - val_fn: 23.0000 - val_accuracy: 0.8438 - val_precision: 0.8882 - val_recall: 0.8544 - val_auc: 0.9075\n",
            "Epoch 106/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7448 - tp: 137.0000 - fp: 40.0000 - tn: 57.0000 - fn: 22.0000 - accuracy: 0.7578 - precision: 0.7740 - recall: 0.8616 - auc: 0.8255 - val_loss: 0.6416 - val_tp: 131.0000 - val_fp: 14.0000 - val_tn: 89.0000 - val_fn: 22.0000 - val_accuracy: 0.8594 - val_precision: 0.9034 - val_recall: 0.8562 - val_auc: 0.9250\n",
            "Epoch 107/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.6872 - tp: 135.0000 - fp: 32.0000 - tn: 61.0000 - fn: 28.0000 - accuracy: 0.7656 - precision: 0.8084 - recall: 0.8282 - auc: 0.8632 - val_loss: 0.6346 - val_tp: 129.0000 - val_fp: 11.0000 - val_tn: 94.0000 - val_fn: 22.0000 - val_accuracy: 0.8711 - val_precision: 0.9214 - val_recall: 0.8543 - val_auc: 0.9312\n",
            "Epoch 108/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6915 - tp: 143.0000 - fp: 26.0000 - tn: 57.0000 - fn: 30.0000 - accuracy: 0.7812 - precision: 0.8462 - recall: 0.8266 - auc: 0.8621 - val_loss: 0.5895 - val_tp: 153.0000 - val_fp: 8.0000 - val_tn: 74.0000 - val_fn: 21.0000 - val_accuracy: 0.8867 - val_precision: 0.9503 - val_recall: 0.8793 - val_auc: 0.9506\n",
            "Epoch 109/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7540 - tp: 140.0000 - fp: 42.0000 - tn: 49.0000 - fn: 25.0000 - accuracy: 0.7383 - precision: 0.7692 - recall: 0.8485 - auc: 0.8228 - val_loss: 0.6351 - val_tp: 136.0000 - val_fp: 18.0000 - val_tn: 79.0000 - val_fn: 23.0000 - val_accuracy: 0.8398 - val_precision: 0.8831 - val_recall: 0.8553 - val_auc: 0.9218\n",
            "Epoch 110/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.7790 - tp: 136.0000 - fp: 33.0000 - tn: 54.0000 - fn: 33.0000 - accuracy: 0.7422 - precision: 0.8047 - recall: 0.8047 - auc: 0.7915 - val_loss: 0.6035 - val_tp: 139.0000 - val_fp: 15.0000 - val_tn: 84.0000 - val_fn: 18.0000 - val_accuracy: 0.8711 - val_precision: 0.9026 - val_recall: 0.8854 - val_auc: 0.9425\n",
            "Epoch 111/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7134 - tp: 148.0000 - fp: 40.0000 - tn: 54.0000 - fn: 14.0000 - accuracy: 0.7891 - precision: 0.7872 - recall: 0.9136 - auc: 0.8463 - val_loss: 0.6478 - val_tp: 148.0000 - val_fp: 22.0000 - val_tn: 68.0000 - val_fn: 18.0000 - val_accuracy: 0.8438 - val_precision: 0.8706 - val_recall: 0.8916 - val_auc: 0.9075\n",
            "Epoch 112/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6904 - tp: 152.0000 - fp: 33.0000 - tn: 53.0000 - fn: 18.0000 - accuracy: 0.8008 - precision: 0.8216 - recall: 0.8941 - auc: 0.8560 - val_loss: 0.6107 - val_tp: 140.0000 - val_fp: 16.0000 - val_tn: 79.0000 - val_fn: 21.0000 - val_accuracy: 0.8555 - val_precision: 0.8974 - val_recall: 0.8696 - val_auc: 0.9181\n",
            "Epoch 113/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7499 - tp: 149.0000 - fp: 35.0000 - tn: 49.0000 - fn: 23.0000 - accuracy: 0.7734 - precision: 0.8098 - recall: 0.8663 - auc: 0.8056 - val_loss: 0.5942 - val_tp: 149.0000 - val_fp: 19.0000 - val_tn: 72.0000 - val_fn: 16.0000 - val_accuracy: 0.8633 - val_precision: 0.8869 - val_recall: 0.9030 - val_auc: 0.9371\n",
            "Epoch 114/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7549 - tp: 134.0000 - fp: 45.0000 - tn: 58.0000 - fn: 19.0000 - accuracy: 0.7500 - precision: 0.7486 - recall: 0.8758 - auc: 0.8299 - val_loss: 0.6356 - val_tp: 135.0000 - val_fp: 18.0000 - val_tn: 85.0000 - val_fn: 18.0000 - val_accuracy: 0.8594 - val_precision: 0.8824 - val_recall: 0.8824 - val_auc: 0.9247\n",
            "Epoch 115/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6715 - tp: 153.0000 - fp: 27.0000 - tn: 55.0000 - fn: 21.0000 - accuracy: 0.8125 - precision: 0.8500 - recall: 0.8793 - auc: 0.8681 - val_loss: 0.6509 - val_tp: 136.0000 - val_fp: 21.0000 - val_tn: 78.0000 - val_fn: 21.0000 - val_accuracy: 0.8359 - val_precision: 0.8662 - val_recall: 0.8662 - val_auc: 0.9103\n",
            "Epoch 116/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.6899 - tp: 134.0000 - fp: 28.0000 - tn: 66.0000 - fn: 28.0000 - accuracy: 0.7812 - precision: 0.8272 - recall: 0.8272 - auc: 0.8595 - val_loss: 0.6362 - val_tp: 129.0000 - val_fp: 18.0000 - val_tn: 81.0000 - val_fn: 28.0000 - val_accuracy: 0.8203 - val_precision: 0.8776 - val_recall: 0.8217 - val_auc: 0.9143\n",
            "Epoch 117/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7195 - tp: 141.0000 - fp: 31.0000 - tn: 62.0000 - fn: 22.0000 - accuracy: 0.7930 - precision: 0.8198 - recall: 0.8650 - auc: 0.8460 - val_loss: 0.6724 - val_tp: 114.0000 - val_fp: 9.0000 - val_tn: 101.0000 - val_fn: 32.0000 - val_accuracy: 0.8398 - val_precision: 0.9268 - val_recall: 0.7808 - val_auc: 0.9096\n",
            "Epoch 118/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7614 - tp: 125.0000 - fp: 38.0000 - tn: 64.0000 - fn: 29.0000 - accuracy: 0.7383 - precision: 0.7669 - recall: 0.8117 - auc: 0.8190 - val_loss: 0.6363 - val_tp: 137.0000 - val_fp: 11.0000 - val_tn: 89.0000 - val_fn: 19.0000 - val_accuracy: 0.8828 - val_precision: 0.9257 - val_recall: 0.8782 - val_auc: 0.9159\n",
            "Epoch 119/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7167 - tp: 143.0000 - fp: 34.0000 - tn: 50.0000 - fn: 29.0000 - accuracy: 0.7539 - precision: 0.8079 - recall: 0.8314 - auc: 0.8248 - val_loss: 0.6269 - val_tp: 136.0000 - val_fp: 15.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8750 - val_precision: 0.9007 - val_recall: 0.8889 - val_auc: 0.9260\n",
            "Epoch 120/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7314 - tp: 141.0000 - fp: 42.0000 - tn: 54.0000 - fn: 19.0000 - accuracy: 0.7617 - precision: 0.7705 - recall: 0.8813 - auc: 0.8419 - val_loss: 0.5905 - val_tp: 147.0000 - val_fp: 18.0000 - val_tn: 73.0000 - val_fn: 18.0000 - val_accuracy: 0.8594 - val_precision: 0.8909 - val_recall: 0.8909 - val_auc: 0.9390\n",
            "Epoch 121/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7218 - tp: 129.0000 - fp: 38.0000 - tn: 67.0000 - fn: 22.0000 - accuracy: 0.7656 - precision: 0.7725 - recall: 0.8543 - auc: 0.8493 - val_loss: 0.5895 - val_tp: 152.0000 - val_fp: 13.0000 - val_tn: 78.0000 - val_fn: 13.0000 - val_accuracy: 0.8984 - val_precision: 0.9212 - val_recall: 0.9212 - val_auc: 0.9471\n",
            "Epoch 122/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6676 - tp: 141.0000 - fp: 31.0000 - tn: 65.0000 - fn: 19.0000 - accuracy: 0.8047 - precision: 0.8198 - recall: 0.8813 - auc: 0.8813 - val_loss: 0.5970 - val_tp: 141.0000 - val_fp: 11.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.9062 - val_precision: 0.9276 - val_recall: 0.9156 - val_auc: 0.9559\n",
            "Epoch 123/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7143 - tp: 129.0000 - fp: 34.0000 - tn: 71.0000 - fn: 22.0000 - accuracy: 0.7812 - precision: 0.7914 - recall: 0.8543 - auc: 0.8592 - val_loss: 0.6041 - val_tp: 149.0000 - val_fp: 11.0000 - val_tn: 77.0000 - val_fn: 19.0000 - val_accuracy: 0.8828 - val_precision: 0.9312 - val_recall: 0.8869 - val_auc: 0.9251\n",
            "Epoch 124/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6824 - tp: 134.0000 - fp: 26.0000 - tn: 71.0000 - fn: 25.0000 - accuracy: 0.8008 - precision: 0.8375 - recall: 0.8428 - auc: 0.8743 - val_loss: 0.6128 - val_tp: 131.0000 - val_fp: 9.0000 - val_tn: 92.0000 - val_fn: 24.0000 - val_accuracy: 0.8711 - val_precision: 0.9357 - val_recall: 0.8452 - val_auc: 0.9332\n",
            "Epoch 125/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.6952 - tp: 133.0000 - fp: 38.0000 - tn: 68.0000 - fn: 17.0000 - accuracy: 0.7852 - precision: 0.7778 - recall: 0.8867 - auc: 0.8777 - val_loss: 0.6040 - val_tp: 146.0000 - val_fp: 9.0000 - val_tn: 82.0000 - val_fn: 19.0000 - val_accuracy: 0.8906 - val_precision: 0.9419 - val_recall: 0.8848 - val_auc: 0.9435\n",
            "Epoch 126/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.8011 - tp: 136.0000 - fp: 45.0000 - tn: 47.0000 - fn: 28.0000 - accuracy: 0.7148 - precision: 0.7514 - recall: 0.8293 - auc: 0.7820 - val_loss: 0.5918 - val_tp: 146.0000 - val_fp: 11.0000 - val_tn: 82.0000 - val_fn: 17.0000 - val_accuracy: 0.8906 - val_precision: 0.9299 - val_recall: 0.8957 - val_auc: 0.9486\n",
            "Epoch 127/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7960 - tp: 121.0000 - fp: 36.0000 - tn: 68.0000 - fn: 31.0000 - accuracy: 0.7383 - precision: 0.7707 - recall: 0.7961 - auc: 0.7928 - val_loss: 0.6288 - val_tp: 135.0000 - val_fp: 7.0000 - val_tn: 87.0000 - val_fn: 27.0000 - val_accuracy: 0.8672 - val_precision: 0.9507 - val_recall: 0.8333 - val_auc: 0.9349\n",
            "Epoch 128/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7247 - tp: 134.0000 - fp: 27.0000 - tn: 64.0000 - fn: 31.0000 - accuracy: 0.7734 - precision: 0.8323 - recall: 0.8121 - auc: 0.8375 - val_loss: 0.6008 - val_tp: 145.0000 - val_fp: 8.0000 - val_tn: 81.0000 - val_fn: 22.0000 - val_accuracy: 0.8828 - val_precision: 0.9477 - val_recall: 0.8683 - val_auc: 0.9441\n",
            "Epoch 129/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7092 - tp: 128.0000 - fp: 38.0000 - tn: 71.0000 - fn: 19.0000 - accuracy: 0.7773 - precision: 0.7711 - recall: 0.8707 - auc: 0.8636 - val_loss: 0.5907 - val_tp: 141.0000 - val_fp: 11.0000 - val_tn: 87.0000 - val_fn: 17.0000 - val_accuracy: 0.8906 - val_precision: 0.9276 - val_recall: 0.8924 - val_auc: 0.9544\n",
            "Epoch 130/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7450 - tp: 124.0000 - fp: 38.0000 - tn: 70.0000 - fn: 24.0000 - accuracy: 0.7578 - precision: 0.7654 - recall: 0.8378 - auc: 0.8352 - val_loss: 0.6192 - val_tp: 146.0000 - val_fp: 21.0000 - val_tn: 74.0000 - val_fn: 15.0000 - val_accuracy: 0.8594 - val_precision: 0.8743 - val_recall: 0.9068 - val_auc: 0.9208\n",
            "Epoch 131/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7133 - tp: 130.0000 - fp: 31.0000 - tn: 69.0000 - fn: 26.0000 - accuracy: 0.7773 - precision: 0.8075 - recall: 0.8333 - auc: 0.8523 - val_loss: 0.6101 - val_tp: 162.0000 - val_fp: 13.0000 - val_tn: 62.0000 - val_fn: 19.0000 - val_accuracy: 0.8750 - val_precision: 0.9257 - val_recall: 0.8950 - val_auc: 0.9200\n",
            "Epoch 132/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7750 - tp: 132.0000 - fp: 39.0000 - tn: 66.0000 - fn: 19.0000 - accuracy: 0.7734 - precision: 0.7719 - recall: 0.8742 - auc: 0.8260 - val_loss: 0.6121 - val_tp: 142.0000 - val_fp: 15.0000 - val_tn: 83.0000 - val_fn: 16.0000 - val_accuracy: 0.8789 - val_precision: 0.9045 - val_recall: 0.8987 - val_auc: 0.9377\n",
            "Epoch 133/200\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.8135 - tp: 123.0000 - fp: 40.0000 - tn: 63.0000 - fn: 30.0000 - accuracy: 0.7266 - precision: 0.7546 - recall: 0.8039 - auc: 0.7956 - val_loss: 0.6448 - val_tp: 131.0000 - val_fp: 14.0000 - val_tn: 85.0000 - val_fn: 26.0000 - val_accuracy: 0.8438 - val_precision: 0.9034 - val_recall: 0.8344 - val_auc: 0.9136\n",
            "Epoch 134/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7431 - tp: 138.0000 - fp: 41.0000 - tn: 51.0000 - fn: 26.0000 - accuracy: 0.7383 - precision: 0.7709 - recall: 0.8415 - auc: 0.8239 - val_loss: 0.5850 - val_tp: 139.0000 - val_fp: 10.0000 - val_tn: 93.0000 - val_fn: 14.0000 - val_accuracy: 0.9062 - val_precision: 0.9329 - val_recall: 0.9085 - val_auc: 0.9562\n",
            "Epoch 135/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6764 - tp: 147.0000 - fp: 22.0000 - tn: 60.0000 - fn: 27.0000 - accuracy: 0.8086 - precision: 0.8698 - recall: 0.8448 - auc: 0.8636 - val_loss: 0.6039 - val_tp: 142.0000 - val_fp: 16.0000 - val_tn: 83.0000 - val_fn: 15.0000 - val_accuracy: 0.8789 - val_precision: 0.8987 - val_recall: 0.9045 - val_auc: 0.9304\n",
            "Epoch 136/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6312 - tp: 154.0000 - fp: 24.0000 - tn: 59.0000 - fn: 19.0000 - accuracy: 0.8320 - precision: 0.8652 - recall: 0.8902 - auc: 0.8988 - val_loss: 0.5645 - val_tp: 154.0000 - val_fp: 10.0000 - val_tn: 76.0000 - val_fn: 16.0000 - val_accuracy: 0.8984 - val_precision: 0.9390 - val_recall: 0.9059 - val_auc: 0.9597\n",
            "Epoch 137/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7254 - tp: 133.0000 - fp: 35.0000 - tn: 69.0000 - fn: 19.0000 - accuracy: 0.7891 - precision: 0.7917 - recall: 0.8750 - auc: 0.8499 - val_loss: 0.6164 - val_tp: 128.0000 - val_fp: 23.0000 - val_tn: 84.0000 - val_fn: 21.0000 - val_accuracy: 0.8281 - val_precision: 0.8477 - val_recall: 0.8591 - val_auc: 0.9298\n",
            "Epoch 138/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7007 - tp: 131.0000 - fp: 34.0000 - tn: 73.0000 - fn: 18.0000 - accuracy: 0.7969 - precision: 0.7939 - recall: 0.8792 - auc: 0.8713 - val_loss: 0.5959 - val_tp: 147.0000 - val_fp: 18.0000 - val_tn: 74.0000 - val_fn: 17.0000 - val_accuracy: 0.8633 - val_precision: 0.8909 - val_recall: 0.8963 - val_auc: 0.9390\n",
            "Epoch 139/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7259 - tp: 134.0000 - fp: 37.0000 - tn: 59.0000 - fn: 26.0000 - accuracy: 0.7539 - precision: 0.7836 - recall: 0.8375 - auc: 0.8348 - val_loss: 0.5699 - val_tp: 144.0000 - val_fp: 12.0000 - val_tn: 84.0000 - val_fn: 16.0000 - val_accuracy: 0.8906 - val_precision: 0.9231 - val_recall: 0.9000 - val_auc: 0.9565\n",
            "Epoch 140/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7440 - tp: 120.0000 - fp: 42.0000 - tn: 71.0000 - fn: 23.0000 - accuracy: 0.7461 - precision: 0.7407 - recall: 0.8392 - auc: 0.8393 - val_loss: 0.5872 - val_tp: 146.0000 - val_fp: 9.0000 - val_tn: 83.0000 - val_fn: 18.0000 - val_accuracy: 0.8945 - val_precision: 0.9419 - val_recall: 0.8902 - val_auc: 0.9394\n",
            "Epoch 141/200\n",
            "8/8 [==============================] - 12s 1s/step - loss: 0.7251 - tp: 142.0000 - fp: 32.0000 - tn: 57.0000 - fn: 25.0000 - accuracy: 0.7773 - precision: 0.8161 - recall: 0.8503 - auc: 0.8330 - val_loss: 0.6242 - val_tp: 134.0000 - val_fp: 18.0000 - val_tn: 82.0000 - val_fn: 22.0000 - val_accuracy: 0.8438 - val_precision: 0.8816 - val_recall: 0.8590 - val_auc: 0.9209\n",
            "Epoch 142/200\n",
            "8/8 [==============================] - 18s 2s/step - loss: 0.7560 - tp: 135.0000 - fp: 36.0000 - tn: 61.0000 - fn: 24.0000 - accuracy: 0.7656 - precision: 0.7895 - recall: 0.8491 - auc: 0.8206 - val_loss: 0.6231 - val_tp: 140.0000 - val_fp: 15.0000 - val_tn: 79.0000 - val_fn: 22.0000 - val_accuracy: 0.8555 - val_precision: 0.9032 - val_recall: 0.8642 - val_auc: 0.9124\n",
            "Epoch 143/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7466 - tp: 126.0000 - fp: 40.0000 - tn: 69.0000 - fn: 21.0000 - accuracy: 0.7617 - precision: 0.7590 - recall: 0.8571 - auc: 0.8399 - val_loss: 0.6117 - val_tp: 146.0000 - val_fp: 21.0000 - val_tn: 72.0000 - val_fn: 17.0000 - val_accuracy: 0.8516 - val_precision: 0.8743 - val_recall: 0.8957 - val_auc: 0.9349\n",
            "Epoch 144/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7117 - tp: 140.0000 - fp: 19.0000 - tn: 64.0000 - fn: 33.0000 - accuracy: 0.7969 - precision: 0.8805 - recall: 0.8092 - auc: 0.8456 - val_loss: 0.5876 - val_tp: 144.0000 - val_fp: 16.0000 - val_tn: 83.0000 - val_fn: 13.0000 - val_accuracy: 0.8867 - val_precision: 0.9000 - val_recall: 0.9172 - val_auc: 0.9577\n",
            "Epoch 145/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6804 - tp: 142.0000 - fp: 33.0000 - tn: 68.0000 - fn: 13.0000 - accuracy: 0.8203 - precision: 0.8114 - recall: 0.9161 - auc: 0.8861 - val_loss: 0.5793 - val_tp: 150.0000 - val_fp: 10.0000 - val_tn: 81.0000 - val_fn: 15.0000 - val_accuracy: 0.9023 - val_precision: 0.9375 - val_recall: 0.9091 - val_auc: 0.9501\n",
            "Epoch 146/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7060 - tp: 137.0000 - fp: 29.0000 - tn: 70.0000 - fn: 20.0000 - accuracy: 0.8086 - precision: 0.8253 - recall: 0.8726 - auc: 0.8587 - val_loss: 0.6233 - val_tp: 135.0000 - val_fp: 15.0000 - val_tn: 87.0000 - val_fn: 19.0000 - val_accuracy: 0.8672 - val_precision: 0.9000 - val_recall: 0.8766 - val_auc: 0.9283\n",
            "Epoch 147/200\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.6981 - tp: 125.0000 - fp: 35.0000 - tn: 74.0000 - fn: 22.0000 - accuracy: 0.7773 - precision: 0.7812 - recall: 0.8503 - auc: 0.8720 - val_loss: 0.6010 - val_tp: 150.0000 - val_fp: 19.0000 - val_tn: 68.0000 - val_fn: 19.0000 - val_accuracy: 0.8516 - val_precision: 0.8876 - val_recall: 0.8876 - val_auc: 0.9308\n",
            "Epoch 148/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6995 - tp: 124.0000 - fp: 29.0000 - tn: 76.0000 - fn: 27.0000 - accuracy: 0.7812 - precision: 0.8105 - recall: 0.8212 - auc: 0.8625 - val_loss: 0.6043 - val_tp: 134.0000 - val_fp: 9.0000 - val_tn: 89.0000 - val_fn: 24.0000 - val_accuracy: 0.8711 - val_precision: 0.9371 - val_recall: 0.8481 - val_auc: 0.9330\n",
            "Epoch 149/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7171 - tp: 126.0000 - fp: 30.0000 - tn: 66.0000 - fn: 34.0000 - accuracy: 0.7500 - precision: 0.8077 - recall: 0.7875 - auc: 0.8434 - val_loss: 0.5644 - val_tp: 154.0000 - val_fp: 15.0000 - val_tn: 70.0000 - val_fn: 17.0000 - val_accuracy: 0.8750 - val_precision: 0.9112 - val_recall: 0.9006 - val_auc: 0.9529\n",
            "Epoch 150/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.6256 - tp: 144.0000 - fp: 16.0000 - tn: 70.0000 - fn: 26.0000 - accuracy: 0.8359 - precision: 0.9000 - recall: 0.8471 - auc: 0.9065 - val_loss: 0.6224 - val_tp: 129.0000 - val_fp: 12.0000 - val_tn: 88.0000 - val_fn: 27.0000 - val_accuracy: 0.8477 - val_precision: 0.9149 - val_recall: 0.8269 - val_auc: 0.9147\n",
            "Epoch 151/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7029 - tp: 134.0000 - fp: 26.0000 - tn: 77.0000 - fn: 19.0000 - accuracy: 0.8242 - precision: 0.8375 - recall: 0.8758 - auc: 0.8665 - val_loss: 0.5918 - val_tp: 137.0000 - val_fp: 14.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8867 - val_precision: 0.9073 - val_recall: 0.9013 - val_auc: 0.9308\n",
            "Epoch 152/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7849 - tp: 127.0000 - fp: 36.0000 - tn: 74.0000 - fn: 19.0000 - accuracy: 0.7852 - precision: 0.7791 - recall: 0.8699 - auc: 0.8372 - val_loss: 0.6088 - val_tp: 141.0000 - val_fp: 13.0000 - val_tn: 82.0000 - val_fn: 20.0000 - val_accuracy: 0.8711 - val_precision: 0.9156 - val_recall: 0.8758 - val_auc: 0.9272\n",
            "Epoch 153/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6601 - tp: 127.0000 - fp: 22.0000 - tn: 80.0000 - fn: 27.0000 - accuracy: 0.8086 - precision: 0.8523 - recall: 0.8247 - auc: 0.8836 - val_loss: 0.5896 - val_tp: 142.0000 - val_fp: 10.0000 - val_tn: 86.0000 - val_fn: 18.0000 - val_accuracy: 0.8906 - val_precision: 0.9342 - val_recall: 0.8875 - val_auc: 0.9417\n",
            "Epoch 154/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7260 - tp: 135.0000 - fp: 32.0000 - tn: 68.0000 - fn: 21.0000 - accuracy: 0.7930 - precision: 0.8084 - recall: 0.8654 - auc: 0.8387 - val_loss: 0.5693 - val_tp: 150.0000 - val_fp: 14.0000 - val_tn: 74.0000 - val_fn: 18.0000 - val_accuracy: 0.8750 - val_precision: 0.9146 - val_recall: 0.8929 - val_auc: 0.9482\n",
            "Epoch 155/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6874 - tp: 135.0000 - fp: 25.0000 - tn: 67.0000 - fn: 29.0000 - accuracy: 0.7891 - precision: 0.8438 - recall: 0.8232 - auc: 0.8678 - val_loss: 0.6097 - val_tp: 129.0000 - val_fp: 9.0000 - val_tn: 94.0000 - val_fn: 24.0000 - val_accuracy: 0.8711 - val_precision: 0.9348 - val_recall: 0.8431 - val_auc: 0.9259\n",
            "Epoch 156/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7258 - tp: 139.0000 - fp: 26.0000 - tn: 61.0000 - fn: 30.0000 - accuracy: 0.7812 - precision: 0.8424 - recall: 0.8225 - auc: 0.8422 - val_loss: 0.5701 - val_tp: 146.0000 - val_fp: 8.0000 - val_tn: 81.0000 - val_fn: 21.0000 - val_accuracy: 0.8867 - val_precision: 0.9481 - val_recall: 0.8743 - val_auc: 0.9486\n",
            "Epoch 157/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7353 - tp: 132.0000 - fp: 36.0000 - tn: 64.0000 - fn: 24.0000 - accuracy: 0.7656 - precision: 0.7857 - recall: 0.8462 - auc: 0.8400 - val_loss: 0.5838 - val_tp: 142.0000 - val_fp: 9.0000 - val_tn: 85.0000 - val_fn: 20.0000 - val_accuracy: 0.8867 - val_precision: 0.9404 - val_recall: 0.8765 - val_auc: 0.9412\n",
            "Epoch 158/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.8054 - tp: 132.0000 - fp: 41.0000 - tn: 58.0000 - fn: 25.0000 - accuracy: 0.7422 - precision: 0.7630 - recall: 0.8408 - auc: 0.7954 - val_loss: 0.5847 - val_tp: 146.0000 - val_fp: 12.0000 - val_tn: 78.0000 - val_fn: 20.0000 - val_accuracy: 0.8750 - val_precision: 0.9241 - val_recall: 0.8795 - val_auc: 0.9354\n",
            "Epoch 159/200\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.7088 - tp: 139.0000 - fp: 34.0000 - tn: 61.0000 - fn: 22.0000 - accuracy: 0.7812 - precision: 0.8035 - recall: 0.8634 - auc: 0.8419 - val_loss: 0.5692 - val_tp: 138.0000 - val_fp: 8.0000 - val_tn: 93.0000 - val_fn: 17.0000 - val_accuracy: 0.9023 - val_precision: 0.9452 - val_recall: 0.8903 - val_auc: 0.9569\n",
            "Epoch 160/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7265 - tp: 141.0000 - fp: 30.0000 - tn: 61.0000 - fn: 24.0000 - accuracy: 0.7891 - precision: 0.8246 - recall: 0.8545 - auc: 0.8305 - val_loss: 0.5827 - val_tp: 154.0000 - val_fp: 13.0000 - val_tn: 68.0000 - val_fn: 21.0000 - val_accuracy: 0.8672 - val_precision: 0.9222 - val_recall: 0.8800 - val_auc: 0.9386\n",
            "Epoch 161/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7300 - tp: 131.0000 - fp: 35.0000 - tn: 63.0000 - fn: 27.0000 - accuracy: 0.7578 - precision: 0.7892 - recall: 0.8291 - auc: 0.8390 - val_loss: 0.5576 - val_tp: 151.0000 - val_fp: 13.0000 - val_tn: 80.0000 - val_fn: 12.0000 - val_accuracy: 0.9023 - val_precision: 0.9207 - val_recall: 0.9264 - val_auc: 0.9606\n",
            "Epoch 162/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7004 - tp: 131.0000 - fp: 36.0000 - tn: 64.0000 - fn: 25.0000 - accuracy: 0.7617 - precision: 0.7844 - recall: 0.8397 - auc: 0.8542 - val_loss: 0.5865 - val_tp: 133.0000 - val_fp: 14.0000 - val_tn: 97.0000 - val_fn: 12.0000 - val_accuracy: 0.8984 - val_precision: 0.9048 - val_recall: 0.9172 - val_auc: 0.9441\n",
            "Epoch 163/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7298 - tp: 131.0000 - fp: 32.0000 - tn: 69.0000 - fn: 24.0000 - accuracy: 0.7812 - precision: 0.8037 - recall: 0.8452 - auc: 0.8415 - val_loss: 0.6041 - val_tp: 140.0000 - val_fp: 15.0000 - val_tn: 82.0000 - val_fn: 19.0000 - val_accuracy: 0.8672 - val_precision: 0.9032 - val_recall: 0.8805 - val_auc: 0.9256\n",
            "Epoch 164/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7073 - tp: 150.0000 - fp: 24.0000 - tn: 48.0000 - fn: 34.0000 - accuracy: 0.7734 - precision: 0.8621 - recall: 0.8152 - auc: 0.8282 - val_loss: 0.5688 - val_tp: 147.0000 - val_fp: 10.0000 - val_tn: 79.0000 - val_fn: 20.0000 - val_accuracy: 0.8828 - val_precision: 0.9363 - val_recall: 0.8802 - val_auc: 0.9440\n",
            "Epoch 165/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7543 - tp: 132.0000 - fp: 36.0000 - tn: 62.0000 - fn: 26.0000 - accuracy: 0.7578 - precision: 0.7857 - recall: 0.8354 - auc: 0.8233 - val_loss: 0.5801 - val_tp: 152.0000 - val_fp: 9.0000 - val_tn: 74.0000 - val_fn: 21.0000 - val_accuracy: 0.8828 - val_precision: 0.9441 - val_recall: 0.8786 - val_auc: 0.9511\n",
            "Epoch 166/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7960 - tp: 120.0000 - fp: 46.0000 - tn: 64.0000 - fn: 26.0000 - accuracy: 0.7188 - precision: 0.7229 - recall: 0.8219 - auc: 0.8029 - val_loss: 0.6151 - val_tp: 134.0000 - val_fp: 16.0000 - val_tn: 80.0000 - val_fn: 26.0000 - val_accuracy: 0.8359 - val_precision: 0.8933 - val_recall: 0.8375 - val_auc: 0.9203\n",
            "Epoch 167/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.6889 - tp: 137.0000 - fp: 31.0000 - tn: 60.0000 - fn: 28.0000 - accuracy: 0.7695 - precision: 0.8155 - recall: 0.8303 - auc: 0.8621 - val_loss: 0.6164 - val_tp: 136.0000 - val_fp: 6.0000 - val_tn: 89.0000 - val_fn: 25.0000 - val_accuracy: 0.8789 - val_precision: 0.9577 - val_recall: 0.8447 - val_auc: 0.9378\n",
            "Epoch 168/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6780 - tp: 130.0000 - fp: 31.0000 - tn: 70.0000 - fn: 25.0000 - accuracy: 0.7812 - precision: 0.8075 - recall: 0.8387 - auc: 0.8663 - val_loss: 0.5562 - val_tp: 146.0000 - val_fp: 9.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.9141 - val_precision: 0.9419 - val_recall: 0.9182 - val_auc: 0.9608\n",
            "Epoch 169/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6778 - tp: 136.0000 - fp: 34.0000 - tn: 66.0000 - fn: 20.0000 - accuracy: 0.7891 - precision: 0.8000 - recall: 0.8718 - auc: 0.8732 - val_loss: 0.5635 - val_tp: 145.0000 - val_fp: 10.0000 - val_tn: 85.0000 - val_fn: 16.0000 - val_accuracy: 0.8984 - val_precision: 0.9355 - val_recall: 0.9006 - val_auc: 0.9525\n",
            "Epoch 170/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7130 - tp: 129.0000 - fp: 33.0000 - tn: 65.0000 - fn: 29.0000 - accuracy: 0.7578 - precision: 0.7963 - recall: 0.8165 - auc: 0.8472 - val_loss: 0.5826 - val_tp: 139.0000 - val_fp: 7.0000 - val_tn: 88.0000 - val_fn: 22.0000 - val_accuracy: 0.8867 - val_precision: 0.9521 - val_recall: 0.8634 - val_auc: 0.9443\n",
            "Epoch 171/200\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.7003 - tp: 140.0000 - fp: 24.0000 - tn: 61.0000 - fn: 31.0000 - accuracy: 0.7852 - precision: 0.8537 - recall: 0.8187 - auc: 0.8515 - val_loss: 0.5552 - val_tp: 146.0000 - val_fp: 7.0000 - val_tn: 87.0000 - val_fn: 16.0000 - val_accuracy: 0.9102 - val_precision: 0.9542 - val_recall: 0.9012 - val_auc: 0.9607\n",
            "Epoch 172/200\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.6845 - tp: 129.0000 - fp: 34.0000 - tn: 67.0000 - fn: 26.0000 - accuracy: 0.7656 - precision: 0.7914 - recall: 0.8323 - auc: 0.8593 - val_loss: 0.5827 - val_tp: 137.0000 - val_fp: 10.0000 - val_tn: 86.0000 - val_fn: 23.0000 - val_accuracy: 0.8711 - val_precision: 0.9320 - val_recall: 0.8562 - val_auc: 0.9523\n",
            "Epoch 173/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7154 - tp: 136.0000 - fp: 33.0000 - tn: 70.0000 - fn: 17.0000 - accuracy: 0.8047 - precision: 0.8047 - recall: 0.8889 - auc: 0.8603 - val_loss: 0.5966 - val_tp: 145.0000 - val_fp: 6.0000 - val_tn: 74.0000 - val_fn: 31.0000 - val_accuracy: 0.8555 - val_precision: 0.9603 - val_recall: 0.8239 - val_auc: 0.9445\n",
            "Epoch 174/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6928 - tp: 145.0000 - fp: 31.0000 - tn: 58.0000 - fn: 22.0000 - accuracy: 0.7930 - precision: 0.8239 - recall: 0.8683 - auc: 0.8537 - val_loss: 0.6144 - val_tp: 122.0000 - val_fp: 4.0000 - val_tn: 99.0000 - val_fn: 31.0000 - val_accuracy: 0.8633 - val_precision: 0.9683 - val_recall: 0.7974 - val_auc: 0.9428\n",
            "Epoch 175/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.8376 - tp: 132.0000 - fp: 30.0000 - tn: 64.0000 - fn: 30.0000 - accuracy: 0.7656 - precision: 0.8148 - recall: 0.8148 - auc: 0.7942 - val_loss: 0.5866 - val_tp: 139.0000 - val_fp: 6.0000 - val_tn: 91.0000 - val_fn: 20.0000 - val_accuracy: 0.8984 - val_precision: 0.9586 - val_recall: 0.8742 - val_auc: 0.9583\n",
            "Epoch 176/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7363 - tp: 135.0000 - fp: 30.0000 - tn: 63.0000 - fn: 28.0000 - accuracy: 0.7734 - precision: 0.8182 - recall: 0.8282 - auc: 0.8301 - val_loss: 0.5935 - val_tp: 146.0000 - val_fp: 8.0000 - val_tn: 82.0000 - val_fn: 20.0000 - val_accuracy: 0.8906 - val_precision: 0.9481 - val_recall: 0.8795 - val_auc: 0.9470\n",
            "Epoch 177/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6790 - tp: 140.0000 - fp: 27.0000 - tn: 71.0000 - fn: 18.0000 - accuracy: 0.8242 - precision: 0.8383 - recall: 0.8861 - auc: 0.8787 - val_loss: 0.5872 - val_tp: 134.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 17.0000 - val_accuracy: 0.8867 - val_precision: 0.9178 - val_recall: 0.8874 - val_auc: 0.9458\n",
            "Epoch 178/200\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.6819 - tp: 135.0000 - fp: 28.0000 - tn: 70.0000 - fn: 23.0000 - accuracy: 0.8008 - precision: 0.8282 - recall: 0.8544 - auc: 0.8676 - val_loss: 0.6085 - val_tp: 140.0000 - val_fp: 12.0000 - val_tn: 81.0000 - val_fn: 23.0000 - val_accuracy: 0.8633 - val_precision: 0.9211 - val_recall: 0.8589 - val_auc: 0.9226\n",
            "Epoch 179/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6909 - tp: 137.0000 - fp: 26.0000 - tn: 58.0000 - fn: 35.0000 - accuracy: 0.7617 - precision: 0.8405 - recall: 0.7965 - auc: 0.8477 - val_loss: 0.5728 - val_tp: 148.0000 - val_fp: 9.0000 - val_tn: 81.0000 - val_fn: 18.0000 - val_accuracy: 0.8945 - val_precision: 0.9427 - val_recall: 0.8916 - val_auc: 0.9538\n",
            "Epoch 180/200\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.8320 - tp: 122.0000 - fp: 42.0000 - tn: 63.0000 - fn: 29.0000 - accuracy: 0.7227 - precision: 0.7439 - recall: 0.8079 - auc: 0.7711 - val_loss: 0.6469 - val_tp: 123.0000 - val_fp: 8.0000 - val_tn: 99.0000 - val_fn: 26.0000 - val_accuracy: 0.8672 - val_precision: 0.9389 - val_recall: 0.8255 - val_auc: 0.9203\n",
            "Epoch 181/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6886 - tp: 140.0000 - fp: 23.0000 - tn: 62.0000 - fn: 31.0000 - accuracy: 0.7891 - precision: 0.8589 - recall: 0.8187 - auc: 0.8552 - val_loss: 0.5771 - val_tp: 138.0000 - val_fp: 10.0000 - val_tn: 97.0000 - val_fn: 11.0000 - val_accuracy: 0.9180 - val_precision: 0.9324 - val_recall: 0.9262 - val_auc: 0.9549\n",
            "Epoch 182/200\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.7171 - tp: 142.0000 - fp: 30.0000 - tn: 64.0000 - fn: 20.0000 - accuracy: 0.8047 - precision: 0.8256 - recall: 0.8765 - auc: 0.8480 - val_loss: 0.5919 - val_tp: 141.0000 - val_fp: 9.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8984 - val_precision: 0.9400 - val_recall: 0.8924 - val_auc: 0.9408\n",
            "Epoch 183/200\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.7131 - tp: 139.0000 - fp: 36.0000 - tn: 60.0000 - fn: 21.0000 - accuracy: 0.7773 - precision: 0.7943 - recall: 0.8687 - auc: 0.8483 - val_loss: 0.6094 - val_tp: 144.0000 - val_fp: 18.0000 - val_tn: 78.0000 - val_fn: 16.0000 - val_accuracy: 0.8672 - val_precision: 0.8889 - val_recall: 0.9000 - val_auc: 0.9280\n",
            "Epoch 184/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7080 - tp: 143.0000 - fp: 36.0000 - tn: 54.0000 - fn: 23.0000 - accuracy: 0.7695 - precision: 0.7989 - recall: 0.8614 - auc: 0.8442 - val_loss: 0.5775 - val_tp: 140.0000 - val_fp: 12.0000 - val_tn: 85.0000 - val_fn: 19.0000 - val_accuracy: 0.8789 - val_precision: 0.9211 - val_recall: 0.8805 - val_auc: 0.9368\n",
            "Epoch 185/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6662 - tp: 140.0000 - fp: 23.0000 - tn: 66.0000 - fn: 27.0000 - accuracy: 0.8047 - precision: 0.8589 - recall: 0.8383 - auc: 0.8739 - val_loss: 0.6092 - val_tp: 136.0000 - val_fp: 20.0000 - val_tn: 86.0000 - val_fn: 14.0000 - val_accuracy: 0.8672 - val_precision: 0.8718 - val_recall: 0.9067 - val_auc: 0.9316\n",
            "Epoch 186/200\n",
            "8/8 [==============================] - 14s 2s/step - loss: 0.7565 - tp: 144.0000 - fp: 32.0000 - tn: 49.0000 - fn: 31.0000 - accuracy: 0.7539 - precision: 0.8182 - recall: 0.8229 - auc: 0.7956 - val_loss: 0.6089 - val_tp: 152.0000 - val_fp: 16.0000 - val_tn: 70.0000 - val_fn: 18.0000 - val_accuracy: 0.8672 - val_precision: 0.9048 - val_recall: 0.8941 - val_auc: 0.9200\n",
            "Epoch 187/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6838 - tp: 139.0000 - fp: 28.0000 - tn: 66.0000 - fn: 23.0000 - accuracy: 0.8008 - precision: 0.8323 - recall: 0.8580 - auc: 0.8625 - val_loss: 0.6076 - val_tp: 140.0000 - val_fp: 13.0000 - val_tn: 82.0000 - val_fn: 21.0000 - val_accuracy: 0.8672 - val_precision: 0.9150 - val_recall: 0.8696 - val_auc: 0.9321\n",
            "Epoch 188/200\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.6838 - tp: 137.0000 - fp: 35.0000 - tn: 60.0000 - fn: 24.0000 - accuracy: 0.7695 - precision: 0.7965 - recall: 0.8509 - auc: 0.8573 - val_loss: 0.5819 - val_tp: 134.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8789 - val_precision: 0.9054 - val_recall: 0.8874 - val_auc: 0.9539\n",
            "Epoch 189/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7240 - tp: 137.0000 - fp: 38.0000 - tn: 60.0000 - fn: 21.0000 - accuracy: 0.7695 - precision: 0.7829 - recall: 0.8671 - auc: 0.8394 - val_loss: 0.6080 - val_tp: 134.0000 - val_fp: 16.0000 - val_tn: 87.0000 - val_fn: 19.0000 - val_accuracy: 0.8633 - val_precision: 0.8933 - val_recall: 0.8758 - val_auc: 0.9409\n",
            "Epoch 190/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6829 - tp: 146.0000 - fp: 30.0000 - tn: 59.0000 - fn: 21.0000 - accuracy: 0.8008 - precision: 0.8295 - recall: 0.8743 - auc: 0.8709 - val_loss: 0.6070 - val_tp: 141.0000 - val_fp: 20.0000 - val_tn: 81.0000 - val_fn: 14.0000 - val_accuracy: 0.8672 - val_precision: 0.8758 - val_recall: 0.9097 - val_auc: 0.9336\n",
            "Epoch 191/200\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.7520 - tp: 140.0000 - fp: 32.0000 - tn: 56.0000 - fn: 28.0000 - accuracy: 0.7656 - precision: 0.8140 - recall: 0.8333 - auc: 0.8089 - val_loss: 0.5992 - val_tp: 152.0000 - val_fp: 16.0000 - val_tn: 76.0000 - val_fn: 12.0000 - val_accuracy: 0.8906 - val_precision: 0.9048 - val_recall: 0.9268 - val_auc: 0.9392\n",
            "Epoch 192/200\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.6994 - tp: 136.0000 - fp: 24.0000 - tn: 65.0000 - fn: 31.0000 - accuracy: 0.7852 - precision: 0.8500 - recall: 0.8144 - auc: 0.8475 - val_loss: 0.6275 - val_tp: 138.0000 - val_fp: 22.0000 - val_tn: 76.0000 - val_fn: 20.0000 - val_accuracy: 0.8359 - val_precision: 0.8625 - val_recall: 0.8734 - val_auc: 0.9079\n",
            "Epoch 193/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7425 - tp: 138.0000 - fp: 30.0000 - tn: 64.0000 - fn: 24.0000 - accuracy: 0.7891 - precision: 0.8214 - recall: 0.8519 - auc: 0.8173 - val_loss: 0.5941 - val_tp: 148.0000 - val_fp: 18.0000 - val_tn: 76.0000 - val_fn: 14.0000 - val_accuracy: 0.8750 - val_precision: 0.8916 - val_recall: 0.9136 - val_auc: 0.9319\n",
            "Epoch 194/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6791 - tp: 142.0000 - fp: 32.0000 - tn: 57.0000 - fn: 25.0000 - accuracy: 0.7773 - precision: 0.8161 - recall: 0.8503 - auc: 0.8552 - val_loss: 0.6151 - val_tp: 137.0000 - val_fp: 20.0000 - val_tn: 82.0000 - val_fn: 17.0000 - val_accuracy: 0.8555 - val_precision: 0.8726 - val_recall: 0.8896 - val_auc: 0.9334\n",
            "Epoch 195/200\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.6554 - tp: 151.0000 - fp: 31.0000 - tn: 50.0000 - fn: 24.0000 - accuracy: 0.7852 - precision: 0.8297 - recall: 0.8629 - auc: 0.8674 - val_loss: 0.5753 - val_tp: 151.0000 - val_fp: 15.0000 - val_tn: 72.0000 - val_fn: 18.0000 - val_accuracy: 0.8711 - val_precision: 0.9096 - val_recall: 0.8935 - val_auc: 0.9423\n",
            "Epoch 196/200\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.6773 - tp: 147.0000 - fp: 36.0000 - tn: 57.0000 - fn: 16.0000 - accuracy: 0.7969 - precision: 0.8033 - recall: 0.9018 - auc: 0.8646 - val_loss: 0.6370 - val_tp: 145.0000 - val_fp: 21.0000 - val_tn: 72.0000 - val_fn: 18.0000 - val_accuracy: 0.8477 - val_precision: 0.8735 - val_recall: 0.8896 - val_auc: 0.8998\n",
            "Epoch 197/200\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.6339 - tp: 150.0000 - fp: 32.0000 - tn: 60.0000 - fn: 14.0000 - accuracy: 0.8203 - precision: 0.8242 - recall: 0.9146 - auc: 0.9012 - val_loss: 0.6077 - val_tp: 144.0000 - val_fp: 14.0000 - val_tn: 78.0000 - val_fn: 20.0000 - val_accuracy: 0.8672 - val_precision: 0.9114 - val_recall: 0.8780 - val_auc: 0.9210\n",
            "Epoch 198/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.7602 - tp: 120.0000 - fp: 52.0000 - tn: 65.0000 - fn: 19.0000 - accuracy: 0.7227 - precision: 0.6977 - recall: 0.8633 - auc: 0.8399 - val_loss: 0.6097 - val_tp: 143.0000 - val_fp: 8.0000 - val_tn: 84.0000 - val_fn: 21.0000 - val_accuracy: 0.8867 - val_precision: 0.9470 - val_recall: 0.8720 - val_auc: 0.9333\n",
            "Epoch 199/200\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.6933 - tp: 131.0000 - fp: 25.0000 - tn: 77.0000 - fn: 23.0000 - accuracy: 0.8125 - precision: 0.8397 - recall: 0.8506 - auc: 0.8632 - val_loss: 0.6040 - val_tp: 113.0000 - val_fp: 5.0000 - val_tn: 107.0000 - val_fn: 31.0000 - val_accuracy: 0.8594 - val_precision: 0.9576 - val_recall: 0.7847 - val_auc: 0.9549\n",
            "Epoch 200/200\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.6928 - tp: 131.0000 - fp: 24.0000 - tn: 80.0000 - fn: 21.0000 - accuracy: 0.8242 - precision: 0.8452 - recall: 0.8618 - auc: 0.8689 - val_loss: 0.5777 - val_tp: 132.0000 - val_fp: 7.0000 - val_tn: 101.0000 - val_fn: 16.0000 - val_accuracy: 0.9102 - val_precision: 0.9496 - val_recall: 0.8919 - val_auc: 0.9568\n",
            "INFO:tensorflow:Assets written to: path_to_my_model_lstm_cnn_89/assets\n",
            "model saved!!!!!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}